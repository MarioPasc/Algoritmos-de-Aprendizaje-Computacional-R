---
title: "Algoritmos de Aprendizaje Computacional"
author: "Mario Pascual González"
format:
  html:
    theme:
      light: flatly
      dark: darkly
    highlight-style: monokai  # Monokai también funciona bien en temas oscuros
    toc: true
    toc-depth: 3
    toc-title: "Contenidos"
    toc-float:
      collapsed: false
      smooth-scroll: true
    toc_scroll: true
    number-sections: true
    code-fold: true
    code-tools: 
      source: true
      toggle: true
      caption: "Expand Code"
      
    html-math-method: katex
    bibliography: references.bib
    lang: es
    other-links:
      - text: LinkedIn
        icon: linkedin
        href: 'https://www.linkedin.com/in/mario-pascual-gonzalez/'
      - text: Correo Electrónico
        icon: envelope
        href: "mailto:mario.pg02@gmail.com?subject=Contacto desde el informe de Modelado Predictivo"
      - text: Perfil de Github
        icon: github
        href: 'https://github.com/MarioPasc'
    code-links:
      - text: Repositorio del Informe
        icon: file-code
        href: 'https://github.com/MarioPasc/Modelado-Predictivo-Cancer-de-Mama-R'
---

```{r setup}
#| output: false
#| echo: false
#| warning: false

library(glmnet)
library(caret)
library(readxl)
library(readr)
library(ggplot2)
library(dplyr)
library(broom)
library(DT)
library(tidyverse)
library(reshape2)
library(MASS)
library(pROC)
library(e1071)
library(nnet)
library(rpart)
library(class)
library(mboost)

data_original <- read.csv(file = "./data/datos_limpios.csv", sep = ",", dec=".")
data_original["X"] <- NULL
nuevo_orden <- c("Edad", "REst", "RPro", "Her2", "Estadio", "NodAfec", "Grado", "Fenotipo", "PCR")
data_original <- data_original[, nuevo_orden]
```

```{r global.options, include = TRUE}
knitr::opts_chunk$set(
    cache       = TRUE,     # if TRUE knitr will cache the results to reuse in future knits
    fig.width   = 7,       # the width for plots created by code chunk
    fig.height  = 4,       # the height for plots created by code chunk
    fig.align   = 'center', # how to align graphics in the final doc. 'left', 'right', 'center'
    fig.path    = 'figs/',  # file path to the directory where knitr shall store the graphics files
    results     = 'asis',   # knitr will pass through results without reformatting them
    echo        = TRUE,     # in FALSE knitr will not display code in the code chunk above it's results
    message     = TRUE,     # if FALSE knitr will not display any messages generated by code
    strip.white = TRUE,     # if FALSE knitr will not remove white spaces at the beg or end of code chunk
    warning     = FALSE)    # if FALSE knitr will not display any warning messages in the final document
```

# Exploración de los Datos

Los datos que con los que se tratará en este documento son datos reales clínicos, provistos por el Dr. Jose Manuel Jerez Aragonés. El fichero fue previamente pre-procesado en el proyecto anterior a este, [Modelado Predictivo para el Cáncer de Mama en R](https://github.com/MarioPasc/Modelado-Predictivo-Cancer-de-Mama-R/tree/main), y, las técnicas usadas incluyen pipelines de imputación de valores faltantes (cuantitavios con la mediana y cualitativos con la moda), manipulación de valores categóricos erróneos, eliminación de *outliers*, y otras técnicas.

## Generación de un conjunto de datos Factor

### Estratificación de la variable Edad

La primera decisión que se ha tomado es la de *estratificar* la variable edad. La anchura de los bins se calculará siguiendo la regla de Sturges, en la que se toma el logaritmo en base 2 de la cantidad de muestras y el rango de la variable para poder computar la anchura más adecuada. 

$$
k = \frac{max(Edad) - min(Edad)}{\log_2(n) + 1}
$$

Finalmente, se pasarán todas las variables a factor. 


```{r}
# Convertir la variable 'Edad' a numérica
data_original$Edad <- as.numeric(data_original$Edad)
data_factor <- data_original

# Calculando el número de bins con la regla de Sturges
n <- nrow(data_factor)
k <- ceiling(log2(n) + 1)

# Calculando el rango y el tamaño de cada bin
minimo <- min(data_factor$Edad)
maximo <- max(data_factor$Edad)
ancho_bin <- (maximo - minimo) / k

# Estratificar la variable 'Edad' en grupos
data_factor$Edad_estratificada <- cut(data_factor$Edad, 
                                      breaks = seq(min(data_factor$Edad), max(data_factor$Edad), by = ancho_bin), 
                                      include.lowest = TRUE, right = FALSE)

# Asignar nombres de grupo a los niveles de Edad_estratificada
levels(data_factor$Edad_estratificada) <- paste0("Grupo", seq_along(levels(data_factor$Edad_estratificada)))

# Convertir todas las variables del dataframe a factor, excepto 'Edad'
data_factor <- data_factor %>%
  mutate_at(vars(-Edad), as.factor)

# Actualizar la variable Edad con los grupos
data_factor$Edad <- data_factor$Edad_estratificada
data_factor$Edad_estratificada <- NULL

knitr::kable(head(data_factor, 10))
```

La estratificación de la variable edad se puede justificar con el hecho de que la interpretación de las predicciones del modelo puede ser más adecuada, ya que no estaríamos hablando de un valor concreto de edad que se repite a la hora de analizar los pacientes PCR-positivos, sino que tal vez se podría identificar una tendencia en los pacientes de un determinado rango de edad a sufrir una metástasis. 

| Grupo   | Rango de Edad |
|---------|----------------|
| Grupo 1 | [24,29.1)      |
| Grupo 2 | [29.1,34.2)    |
| Grupo 3 | [34.2,39.3)    |
| Grupo 4 | [39.3,44.4)    |
| Grupo 5 | [44.4,49.5)    |
| Grupo 6 | [49.5,54.6)    |
| Grupo 7 | [54.6,59.7)    |
| Grupo 8 | [59.7,64.8)    |
| Grupo 9 | [64.8,69.9)    |
| Grupo 10| [69.9,75)      |




### Distribución de los Datos para Factor

```{r}
blue = '#377eb8'
red = '#e41a1c'
plot_variable_distribution <- function(data, variable_name, target_name) {
  # Comprobar si las variables existen en el data.frame
  if (!(variable_name %in% names(data) && target_name %in% names(data))) {
    stop("Una o ambas variables especificadas no existen en el data.frame proporcionado.")
  }
  
  # Extraer la variable y la variable target del data.frame
  variable <- data[[variable_name]]
  target <- data[[target_name]]
  
  # Asegurar que la variable target es factor
  data[[target_name]] <- as.factor(data[[target_name]])
  
  # Determinar si la variable principal es numérica o factor
  if (is.numeric(variable)) {
    # Crear un histograma para variables numéricas
    p <- ggplot(data, aes_string(x = variable_name, fill = target_name)) +
      geom_histogram(position = "identity", alpha = 0.5, bins = 30) +
      scale_fill_manual(values = c("0" = blue, "1" = red)) +
      ggtitle(paste("Distribución de", variable_name, "por", target_name)) +
      xlab(variable_name) +
      ylab("Frecuencia")
  } else if (is.factor(variable)) {
    # Crear un gráfico de barras para factores
    p <- ggplot(data, aes_string(x = variable_name, fill = target_name)) +
      geom_bar(position = "stack") +
      scale_fill_manual(values = c("0" = blue, "1" = red)) +
      ggtitle(paste("Distribución de", variable_name, "por", target_name)) +
      xlab(variable_name) +
      ylab("Frecuencia") +
      theme(legend.position = "right")
  } else {
    stop("El tipo de la variable debe ser numérico o factor.")
  }
  
  # Imprimir el gráfico
  print(p)
}

```

::: {.panel-tabset}

## Edad

```{r}
plot_variable_distribution(data_factor, "Edad", "PCR")
```

## REst

```{r}
plot_variable_distribution(data_factor, "REst", "PCR")
```

## RPro

```{r}
plot_variable_distribution(data_factor, "RPro", "PCR")
```

## Her2

```{r}
plot_variable_distribution(data_factor, "Her2", "PCR")
```

## Estadio

```{r}
plot_variable_distribution(data_factor, "Estadio", "PCR")
```

## NodAfec

```{r}
plot_variable_distribution(data_factor, "NodAfec", "PCR")
```

## Grado

```{r}
plot_variable_distribution(data_factor, "Grado", "PCR")
```

## Fenotipo

```{r}
plot_variable_distribution(data_factor, "Fenotipo", "PCR")
```

## PCR

```{r}
plot_variable_distribution(data_factor, "PCR", "PCR")
```

:::

Como se puede observar en las gráficas anteriores, el conjunto de datos se encuentra **desbalanceado** con respecto a la variable objetivo, PCR, a lo largo de todas las demás variables del conjunto de datos. Esto se evidencia al visualizar la propia distribución de la variable PCR, la cual muestra la falta de entradas positivas, algo normal en los estudios clínicos en los que una connotación negativa está asociada a la muestra positiva -en este caso, la metástasis del cáncer. 


Para poder tener en cuenta cómo estos datos se relacionan con la variable objetivo, y así poder determinar si la excasa cantidad de estos afecta de una manera muy negativa la predicción de nuestro modelo, se realizará un *análisis de asociación*. 

## Generación de un conjunto de datos numérico

```{r}
encode_data <- function(data, target_name) {
  # Convertir variables categóricas sin orden en dummies
  data <- dummyVars(~ ., data = data, fullRank = TRUE) %>% predict(data) %>% as.data.frame()
  # Asegurar que la variable objetivo esté al final
  target <- data[[target_name]]
  data[[target_name]] <- NULL
  data[[target_name]] <- target
  return(data)
}

data_numeric <- encode_data(data_original, "PCR")
knitr::kable(head(data_numeric, 10))
```

# Selección de características

## Variables demasiado desbalanceadas

Las variables con un desbalanceo de clases demasiado significativo podrían introducir un sesgo a la hora de realizar la división entre los conjuntos de datos de entrenamiento, validación y prueba. Debido a esto, se tomarán las siguientes precauciones:

1. Si una variable es bicategórica y desbalanceada, se eliminará del conjunto de datos. Este podría ser el caso de `Her2`. 

```{r}
data_numeric$Her2P <- NULL
data_factor$Her2 <- NULL
data_original$Her2 <- NULL
```


2. Si una variable no es bicategórica y está desbalanceada, siempre que se mantenga el sentido clínico, se fusionarán dos de sus categorías en una. 

## Métodos de Filtrado: Análisis de Asociación

```{r, warning=FALSE, message=FALSE}
source("./aux_scripts/calculaPValor.R")
plot <- plot_p_valores(data_original)
print(plot)
```

Como se puede observar en la salida del modelo de regresión logística, las variables que muestran una asociación estadísticamente significativa con la variable objetivo PCR son **Estadio, Fenotipo, Grado, REst, y RPro**. Estas variables han sido seleccionadas mediante un proceso iterativo que utiliza el algoritmo `stepAIC`, el cual optimiza el modelo añadiendo o eliminando variables para minimizar el criterio de información de Akaike (AIC). Este enfoque no solo confirma la significancia estadística inicial observada en los análisis de p-valores, sino que también ajusta el modelo para incluir solo las variables más informativas.

```{r}
source("./aux_scripts/calculaPValor.R")

ajustado <- ajustarModeloLogistico(data, "PCR", "Estadio + Fenotipo + Grado + REst + RPro")
asociacion <- c("Estadio", "Fenotipo", "Grado", "REst", "RPro")
variables_seleccionadas <- ajustado$variables_seleccionadas
variables_asociacion <- "Estadio+Fenotipo+Grado+REst+RPro"
```

El modelo final selecciona múltiples categorías dentro de las variables Estadio, Fenotipo, Grado, REst, y RPro que son determinantes para explicar la variable PCR. Estas categorías incluyen, por ejemplo, diferentes tipos de Estadio y Fenotipo, que muestran cómo varía la probabilidad de PCR en función de estas características. Este conjunto de variables y categorías será denominado de ahora en adelante como Asociacion, en referencia al método efectivo de selección de variables utilizado.


## Métodos de Wrapped: StepAUC Backwards


```{r}
load("RData_Files_Algorithms/NaiveBayes_StepAUC.RData")
resultadosAparentesNaiveBayes
```


# Modelos Predictivos Avanzados

El principal objetivo de este proyecto es el de poder predecir si un paciente será PCR-Positivo (Metástasis) mediante el uso de Algoritmos de Aprendizaje Computacional para el procesamiento de los datos de cáncer de mama previamente curados. Para poder conseguir el modelo que más se ajuste a esta tarea, se debe realizar una selección de características sobre el conjunto de datos y un ajuste fino (*fine-tuning*) de los parámetros del modelo de predicción. Todo esto debe ser llevado a cabo utilizando métodos de validación interna que eliminen cualquier tipo de sesgo inherente dentro de los datos en la variable objetivo, asegurando unos resultados estables para la evaluación del rendimiento del modelo para datos a futuro.  

## Rendimiento Aparente

Para poder realizar una estimación de cómo de bien los modelos conseguirán predecir sobre el conjunto de datos se va a realizar una estimación del rendimiento aparente. Esto consistirá en entrenar con todo el conjunto de datos, y evaluar con estos mismos datos. De esta forma, podremos evaluar la capacidad predictiva del modelo antes de intentar encontrar una manera estable de obtener los métricas de rendimiento -es decir, aplicar una validación interna.

### Máquinas de Vectores de Soporte (SVM)

```{r, warning=FALSE, message=FALSE}
evaluate_aparent_performance_model_svm <- function(data, target_var, model_func, vars = NULL, threshold = 0.5) {
  if (is.null(vars)) {
    vars <- setdiff(names(data), target_var)
  }
  
  data[[target_var]] <- factor(data[[target_var]], levels = c(0, 1))
  formula <- as.formula(paste(target_var, "~", paste(vars, collapse = "+")))
  
  model <- model_func(formula, data)
  predictions <- predict(model, newdata = data, probability = TRUE)
  predictions <- attr(predictions, "probabilities")[, 2]
  
  actual_classes <- data[[target_var]]
  predicted_classes <- ifelse(predictions > threshold, 1, 0)
  confusion_matrix <- table(predicted_classes, actual_classes)
  
  tp <- ifelse("1" %in% rownames(confusion_matrix) && "1" %in% colnames(confusion_matrix), confusion_matrix["1", "1"], 0)
  tn <- ifelse("0" %in% rownames(confusion_matrix) && "0" %in% colnames(confusion_matrix), confusion_matrix["0", "0"], 0)
  fp <- ifelse("1" %in% rownames(confusion_matrix) && "0" %in% colnames(confusion_matrix), confusion_matrix["1", "0"], 0)
  fn <- ifelse("0" %in% rownames(confusion_matrix) && "1" %in% colnames(confusion_matrix), confusion_matrix["0", "1"], 0)
  
  accuracy <- (tp + tn) / (tp + tn + fp + fn)
  precision <- ifelse(tp + fp > 0, tp / (tp + fp), 0)
  recall <- ifelse(tp + fn > 0, tp / (tp + fn), 0)
  f1_score <- ifelse(precision + recall > 0, 2 * (precision * recall) / (precision + recall), 0)
  roc_obj <- pROC::roc(actual_classes, predictions)
  
  list(confusion_matrix = confusion_matrix, accuracy = accuracy, precision = precision, 
       recall = recall, f1_score = f1_score, roc_curve = roc_obj)
}


svm_model <- function(formula, data) {
  set.seed(90)
  x <- model.matrix(formula, data)
  y <- data[[all.vars(formula)[1]]]
  y <- factor(y, levels = c(0, 1))
  svm(x = x, y = y, kernel = "linear", cost = 100, probability = TRUE, maxiter=300, random_state=90)
}
# svm(x = x, y = y, kernel = "poly", cost = 5, gamma = .5, probability = TRUE, maxiter=150)
# svm(x = x, y = y, kernel = "linear", cost = 50, probability = TRUE, maxiter=300, random_state=90)


resultadosAparentesSVM <- evaluate_aparent_performance_model_svm(data = data_numeric, target_var = "PCR",
                                                             model_func = svm_model,
                                                             vars = ".",
                                                             threshold = .35)
resultadosAparentesSVM$confusion_matrix
print("Accuracy: ")
resultadosAparentesSVM$accuracy
print("Precision: ")
resultadosAparentesSVM$precision
print("Recall: ")
resultadosAparentesSVM$recall
print("F1-Score: ")
resultadosAparentesSVM$f1_score
resultadosAparentesSVM$roc_curve$auc

```



### Redes de Neuronas Artificiales (ANN)

```{r, message=FALSE, warining=FALSE}
evaluate_aparent_performance_model_nnet <- function(data, target_var, model_func, vars = NULL, threshold = 0.5) {
  if (is.null(vars)) {
    vars <- setdiff(names(data), target_var)
  }
  
  data[[target_var]] <- factor(data[[target_var]], levels = c(0, 1))
  formula <- as.formula(paste(target_var, "~", paste(vars, collapse = "+")))
  
  model <- model_func(formula, data)
  predictions <- predict(model, newdata = data, type = "raw")
  
  actual_classes <- data[[target_var]]
  predicted_classes <- ifelse(predictions > threshold, 1, 0)
  confusion_matrix <- table(predicted_classes, actual_classes)
  
  tp <- ifelse("1" %in% rownames(confusion_matrix) && "1" %in% colnames(confusion_matrix), confusion_matrix["1", "1"], 0)
  tn <- ifelse("0" %in% rownames(confusion_matrix) && "0" %in% colnames(confusion_matrix), confusion_matrix["0", "0"], 0)
  fp <- ifelse("1" %in% rownames(confusion_matrix) && "0" %in% colnames(confusion_matrix), confusion_matrix["1", "0"], 0)
  fn <- ifelse("0" %in% rownames(confusion_matrix) && "1" %in% colnames(confusion_matrix), confusion_matrix["0", "1"], 0)
  
  accuracy <- (tp + tn) / (tp + tn + fp + fn)
  precision <- ifelse(tp + fp > 0, tp / (tp + fp), 0)
  recall <- ifelse(tp + fn > 0, tp / (tp + fn), 0)
  f1_score <- ifelse(precision + recall > 0, 2 * (precision * recall) / (precision + recall), 0)
  roc_obj <- pROC::roc(actual_classes, predictions)
  
  list(confusion_matrix = confusion_matrix, accuracy = accuracy, precision = precision, 
       recall = recall, f1_score = f1_score, roc_curve = roc_obj)
}


nn_model <- function(formula, data) {
  set.seed(90)
  nnet(formula = formula, data = data, size = 15, 
       decay = 0.1, maxit = 100, trace = FALSE, 
       linout = FALSE, random_state = 90)
}


resultadosAparentesNN <- evaluate_aparent_performance_model_nnet(data = data_factor, target_var = "PCR",
                                                             model_func = nn_model,
                                                             vars = ".",
                                                             threshold = .35)
resultadosAparentesNN$confusion_matrix
resultadosAparentesNN$roc_curve$auc
print("Accuracy: ")
resultadosAparentesNN$accuracy
print("Precision: ")
resultadosAparentesNN$precision
print("Recall: ")
resultadosAparentesNN$recall
print("F1-Score: ")
resultadosAparentesNN$f1_score
print("AUC: ")
resultadosAparentesNN$roc_curve$auc
```
### Bosques Aleatorios (Random Forest, RF)

```{r}
evaluate_aparent_performance_model_rpart <- function(data, target_var, model_func, vars = NULL, threshold = 0.5) {
  if (is.null(vars)) {
    vars <- setdiff(names(data), target_var)
  }
  
  data[[target_var]] <- factor(data[[target_var]], levels = c(0, 1))
  formula <- as.formula(paste(target_var, "~", paste(vars, collapse = "+")))
  
  model <- model_func(formula, data)
  predictions <- predict(model, newdata = data, type = "prob")[, 2]
  
  actual_classes <- data[[target_var]]
  predicted_classes <- ifelse(predictions > threshold, 1, 0)
  confusion_matrix <- table(predicted_classes, actual_classes)
  
  tp <- ifelse("1" %in% rownames(confusion_matrix) && "1" %in% colnames(confusion_matrix), confusion_matrix["1", "1"], 0)
  tn <- ifelse("0" %in% rownames(confusion_matrix) && "0" %in% colnames(confusion_matrix), confusion_matrix["0", "0"], 0)
  fp <- ifelse("1" %in% rownames(confusion_matrix) && "0" %in% colnames(confusion_matrix), confusion_matrix["1", "0"], 0)
  fn <- ifelse("0" %in% rownames(confusion_matrix) && "1" %in% colnames(confusion_matrix), confusion_matrix["0", "1"], 0)
  
  accuracy <- (tp + tn) / (tp + tn + fp + fn)
  precision <- ifelse(tp + fp > 0, tp / (tp + fp), 0)
  recall <- ifelse(tp + fn > 0, tp / (tp + fn), 0)
  f1_score <- ifelse(precision + recall > 0, 2 * (precision * recall) / (precision + recall), 0)
  roc_obj <- pROC::roc(actual_classes, predictions)
  
  list(confusion_matrix = confusion_matrix, accuracy = accuracy, precision = precision, 
       recall = recall, f1_score = f1_score, roc_curve = roc_obj)
}


rf_model <- function(formula, data) {
  set.seed(90)
  rpart(formula = formula, data = data, method = "class", control = rpart.control(minsplit = 2, cp = 0))
}


resultadosAparentesRF <- evaluate_aparent_performance_model_rpart(data = data_factor, target_var = "PCR",
                                                             model_func = rf_model,
                                                             vars = ".",
                                                             threshold = .35)
resultadosAparentesRF$confusion_matrix
resultadosAparentesRF$roc_curve$auc
print("Accuracy: ")
resultadosAparentesRF$accuracy
print("Precision: ")
resultadosAparentesRF$precision
print("Recall: ")
resultadosAparentesRF$recall
print("F1-Score: ")
resultadosAparentesRF$f1_score
print("AUC: ")
resultadosAparentesRF$roc_curve$auc
```
### K-Vecinos Cercanos (KNN)

```{r}
evaluate_aparent_performance_model_knn <- function(data, target_var, model_func, vars = NULL, threshold = 0.5) {
  if (is.null(vars)) {
    vars <- setdiff(names(data), target_var)
  }
  
  data[[target_var]] <- factor(data[[target_var]], levels = c(0, 1))
  formula <- as.formula(paste(target_var, "~", paste(vars, collapse = "+")))
  
  model <- model_func(formula, data)
  train_x <- model$train_x
  train_y <- model$train_y
  test_x <- model.matrix(formula, data)[, -1]
  predictions <- knn(train = train_x, test = test_x, cl = train_y, k = model$k, prob = TRUE)
  predictions <- attr(predictions, "prob")
  
  actual_classes <- data[[target_var]]
  predicted_classes <- ifelse(predictions > threshold, 1, 0)
  confusion_matrix <- table(predicted_classes, actual_classes)
  
  tp <- ifelse("1" %in% rownames(confusion_matrix) && "1" %in% colnames(confusion_matrix), confusion_matrix["1", "1"], 0)
  tn <- ifelse("0" %in% rownames(confusion_matrix) && "0" %in% colnames(confusion_matrix), confusion_matrix["0", "0"], 0)
  fp <- ifelse("1" %in% rownames(confusion_matrix) && "0" %in% colnames(confusion_matrix), confusion_matrix["1", "0"], 0)
  fn <- ifelse("0" %in% rownames(confusion_matrix) && "1" %in% colnames(confusion_matrix), confusion_matrix["0", "1"], 0)
  
  accuracy <- (tp + tn) / (tp + tn + fp + fn)
  precision <- ifelse(tp + fp > 0, tp / (tp + fp), 0)
  recall <- ifelse(tp + fn > 0, tp / (tp + fn), 0)
  f1_score <- ifelse(precision + recall > 0, 2 * (precision * recall) / (precision + recall), 0)
  roc_obj <- pROC::roc(actual_classes, predictions)
  
  list(confusion_matrix = confusion_matrix, accuracy = accuracy, precision = precision, 
       recall = recall, f1_score = f1_score, roc_curve = roc_obj)
}


# Ejemplo de llamada para KNN
knn_model <- function(formula, data) {
  train_x <- model.matrix(formula, data)[, -1]
  train_y <- data[[all.vars(formula)[1]]]
  list(train_x = train_x, train_y = train_y, k = 5)  # Ajusta el valor de k según sea necesario
}

resultadosAparentesKNN <- evaluate_aparent_performance_model_knn(data = data_numeric, target_var = "PCR",
                                                             model_func = knn_model,
                                                             vars = ".",
                                                             threshold = .35)
resultadosAparentesKNN$confusion_matrix
resultadosAparentesKNN$roc_curve$auc
print("Accuracy: ")
resultadosAparentesKNN$accuracy
print("Precision: ")
resultadosAparentesKNN$precision
print("Recall: ")
resultadosAparentesKNN$recall
print("F1-Score: ")
resultadosAparentesKNN$f1_score
print("AUC: ")
resultadosAparentesKNN$roc_curve$auc
```

### Adaboost 

```{r}
evaluate_aparent_performance_model_adaboost <- function(data, target_var, model_func, vars = NULL, iterations = 100, threshold = 0.5) {
  if (is.null(vars)) {
    vars <- setdiff(names(data), target_var)
  }
  
  data[[target_var]] <- factor(data[[target_var]], levels = c(0, 1))
  formula <- as.formula(paste(target_var, "~", paste(vars, collapse = "+")))
  
  model <- model_func(formula, data, iterations)
  predictions <- predict(model, newdata = data, type = "response")
  
  actual_classes <- data[[target_var]]
  predicted_classes <- ifelse(predictions > threshold, 1, 0)
  confusion_matrix <- table(Predicted = predicted_classes, Actual = actual_classes)
  
  tp <- ifelse("1" %in% rownames(confusion_matrix) && "1" %in% colnames(confusion_matrix), confusion_matrix["1", "1"], 0)
  tn <- ifelse("0" %in% rownames(confusion_matrix) && "0" %in% colnames(confusion_matrix), confusion_matrix["0", "0"], 0)
  fp <- ifelse("1" %in% rownames(confusion_matrix) && "0" %in% colnames(confusion_matrix), confusion_matrix["1", "0"], 0)
  fn <- ifelse("0" %in% rownames(confusion_matrix) && "1" %in% colnames(confusion_matrix), confusion_matrix["0", "1"], 0)
  
  accuracy <- (tp + tn) / (tp + tn + fp + fn)
  precision <- ifelse(tp + fp > 0, tp / (tp + fp), 0)
  recall <- ifelse(tp + fn > 0, tp / (tp + fn), 0)
  f1_score <- ifelse(precision + recall > 0, 2 * (precision * recall) / (precision + recall), 0)
  roc_obj <- pROC::roc(actual_classes, predictions)
  
  list(confusion_matrix = confusion_matrix, accuracy = accuracy, precision = precision, 
       recall = recall, f1_score = f1_score, roc_curve = roc_obj)
}

adaboost_model <- function(formula, data, iterations) {
  model <- glmboost(formula, data = data, family = Binomial(), control = boost_control(mstop = iterations))
  model
}

resultadosAparentesAdaBoost <- evaluate_aparent_performance_model_adaboost(data = data_numeric, target_var = "PCR",
                                                                           model_func = adaboost_model,
                                                                           vars = ".",
                                                                           iterations = 100,
                                                                           threshold = 0.35)

print(resultadosAparentesAdaBoost$confusion_matrix)
print(paste("Accuracy: ", resultadosAparentesAdaBoost$accuracy))
print(paste("Precision: ", resultadosAparentesAdaBoost$precision))
print(paste("Recall: ", resultadosAparentesAdaBoost$recall))
print(paste("F1-Score: ", resultadosAparentesAdaBoost$f1_score))
print(paste("AUC: ", resultadosAparentesAdaBoost$roc_curve$auc))



```

### Naive Bayes

```{r}
evaluate_aparent_performance_model_naiveBayes <- function(data, target_var, model_func, vars = NULL, threshold = 0.5) {
  if (is.null(vars)) {
    vars <- setdiff(names(data), target_var)
  }
  
  data[[target_var]] <- factor(data[[target_var]], levels = c(0, 1))
  formula <- as.formula(paste(target_var, "~", paste(vars, collapse = "+")))
  
  model <- model_func(formula, data)
  predictions <- predict(model, newdata = data, type = "raw")[, 2]
  
  actual_classes <- data[[target_var]]
  predicted_classes <- ifelse(predictions > threshold, 1, 0)
  confusion_matrix <- table(predicted_classes, actual_classes)
  
  tp <- ifelse("1" %in% rownames(confusion_matrix) && "1" %in% colnames(confusion_matrix), confusion_matrix["1", "1"], 0)
  tn <- ifelse("0" %in% rownames(confusion_matrix) && "0" %in% colnames(confusion_matrix), confusion_matrix["0", "0"], 0)
  fp <- ifelse("1" %in% rownames(confusion_matrix) && "0" %in% colnames(confusion_matrix), confusion_matrix["1", "0"], 0)
  fn <- ifelse("0" %in% rownames(confusion_matrix) && "1" %in% colnames(confusion_matrix), confusion_matrix["0", "1"], 0)
  
  accuracy <- (tp + tn) / (tp + tn + fp + fn)
  precision <- ifelse(tp + fp > 0, tp / (tp + fp), 0)
  recall <- ifelse(tp + fn > 0, tp / (tp + fn), 0)
  f1_score <- ifelse(precision + recall > 0, 2 * (precision * recall) / (precision + recall), 0)
  roc_obj <- pROC::roc(actual_classes, predictions)
  
  list(confusion_matrix = confusion_matrix, accuracy = accuracy, precision = precision, 
       recall = recall, f1_score = f1_score, roc_curve = roc_obj)
}

# Ejemplo de llamada para Naive Bayes
naiveBayes_model <- function(formula, data) {
  naiveBayes(formula = formula, data = data)
}

resultadosAparentesNaiveBayes <- evaluate_aparent_performance_model_naiveBayes(data = data_factor, target_var = "PCR",
                                                                    model_func = naiveBayes_model,
                                                                    vars = ".",
                                                                    threshold = .35)
resultadosAparentesNaiveBayes$confusion_matrix
resultadosAparentesNaiveBayes$roc_curve$auc
print("Accuracy: ")
resultadosAparentesNaiveBayes$accuracy
print("Precision: ")
resultadosAparentesNaiveBayes$precision
print("Recall: ")
resultadosAparentesNaiveBayes$recall
print("F1-Score: ")
resultadosAparentesNaiveBayes$f1_score
print("AUC: ")
resultadosAparentesNaiveBayes$roc_curve$auc
```





## Validación Interna

### División de los datos

Como se ha visto anteriormente, el conjunto de datos sufre de un desbalance en su variable objetivo, PCR. Si bien se evaluará la capacidad predictiva de cada modelo entrenando y evaluando con todos los datos, se necesitará alguna forma de cuantificar el rendimiento del modelo para datos a futuro. 

Debido a que los modelos con los que se va a trabajar a continuación son algoritmos complejos y con una etapa de entrenamiento y ajuste fino complejas, se nos introduce la necesidad de dividir el conjunto de datos en 3 conjuntos: *entrenamiento, validación* y *test*. La justificación de no realizar simplemente una división de *train* y *test* reside en que cuando solo se utiliza un conjunto de entrenamiento y uno de prueba, existe el riesgo de que el ajuste fino del modelo se haga específicamente para maximizar el rendimiento en el conjunto de prueba. Esto puede llevar a un modelo que esté sobreajustado a las características específicas de este conjunto de datos, lo que disminuye su capacidad para generalizar a nuevos datos. El conjunto de validación permitirá entonces ajustar los hiperparámetros y hacer selecciones de modelo sin "contaminar" el conjunto de prueba, que se reserva para una evaluación final más objetiva.

Adicionalmente, mantener el conjunto de prueba completamente independiente de cualquier decisión tomada durante el proceso de modelado asegura que el rendimiento evaluado en este conjunto es una representación honesta y no sesgada de cómo el modelo se comportará con datos nuevos y no vistos.

Esto se realizará guardando un conjunto de test totalmente separado del conjunto de entrenamiento. Este conjunto de test será "olvidado" hasta que se tenga que hacer la evaluación final de los modelos. El conjunto de entrenamiento se dividirá en validación, y, entrenamiento. Esta división se realizará dentro del modelo de validación interna que se decida usar. 

```{r}
source("./aux_scripts/procedimientosMachineLearning.R")
splitted_data <- train_test_split(data, 0.8, "PCR", "./data")

train_index <- splitted_data$train_index
plot <- splitted_data$plot
summary <- splitted_data$summary

train_data <- read.csv(file = "./data/train.csv", sep = ",", dec=".")
test_data <- read.csv(file = "./data/test.csv", sep = ",", dec=".")
```

::: {.panel-tabset}

## Gráfico de distribución de PCR en los diferentes conjuntos

```{r}
print(plot)
```

## Resumen de distribución de datos a través de conjuntos

```{r}
knitr::kable(summary)
```
:::

### Métodos de validación interna

Como se demuestra en [@gonzalez2024predictive], un estudio anterior realizado utilizando este mismo conjunto de datos para el diseño de un modelo pridctivo basado en la regresión logística, el método recomendado por la literatura, *k-fold cross validation* no funciona de una manera efectiva con estas muestras. Esto se debe a que el desbalance de la variable objetivo y la moderada cantidad de datos hace que el rendimiento del modelo se vea limitado al elegir un k demasiado bajo, ya que las métricas se vuelven poco representativas, y un k demasiado alto, ya que los folds se vuelven poco representativos al disminuir la varianza de los datos de cada uno, haciendo que el rendimiento se vuelva inestable. 

Es por esto que, a lo largo de este estudio, se utilizará un método de validación basado en *Repeated Hold-Out* -con la misma semilla para asegurar la reproducibilidad. Para poder obtener un rendimiento representativo sin repetir divisiones de datos aleatorias, se harán 30 iteraciones. 

## Regresión Logística

```{r}
library(caret)
library(pROC)

# Función para realizar doble validación cruzada 5x2 con regresión logística
double_cross_validation_glm <- function(data, target_name, outer, inner) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(), FP = integer(), FN = integer(), AUC = numeric(), stringsAsFactors = FALSE)

  outer_folds <- createFolds(data[[target_name]], k = outer)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data[-outer_folds[[outer_index]], ]
    outer_test_data <- data[outer_folds[[outer_index]], ]

    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner)
    best_model <- NULL
    best_auc <- 0

    for (inner_index in seq_along(inner_folds)) {
      inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
      inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]

      # Entrenar el modelo de regresión logística
      formula <- reformulate(colnames(data)[!colnames(data) %in% target_name], target_name)
      model <- glm(formula, data = inner_train_data, family = binomial())

      # Evaluar el modelo
      pred <- predict(model, inner_test_data, type = "response")
      roc_curve <- pROC::roc(inner_test_data[[target_name]], pred)
      auc <- roc_curve$auc

      # Guardar el mejor modelo basado en AUC
      if (auc > best_auc) {
        best_auc <- auc
        best_model <- model
      }
    }

    # Evaluar el mejor modelo en el outer test fold
    predictions <- predict(best_model, outer_test_data, type = "response")
    pred_class <- ifelse(predictions > 0.5, 1, 0)  # Asignar clases basadas en umbral
    confusion <- table(Actual = outer_test_data[[target_name]], Predicted = pred_class)
    TP <- confusion[2, 2]
    TN <- confusion[1, 1]
    FP <- confusion[1, 2]
    FN <- confusion[2, 1]
    auc <- pROC::roc(outer_test_data[[target_name]], predictions)$auc

    # Registrar las métricas
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc), names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
  }
  
  return(performance_metrics)
}

performance_glm <- double_cross_validation_glm(data=data,
                                               target_name = "PCR",
                                               outer=5,
                                               inner=5)
performance_glm
```


## Máquinas de Vectores de Soportte (SVM)

Una Máquina de Vectores de Soporte (SVM) es un algoritmo de aprendizaje supervisado utilizado para la clasificación y regresión. Su objetivo principal es encontrar el **hiperplano óptimo** que mejor separa las clases en el espacio de características. Los **vectores de soporte** son los puntos de datos más cercanos al hiperplano de separación y son cruciales para determinar su posición y orientación. Estos vectores de soporte son los puntos que definen el *"margen"*, que es la distancia entre el hiperplano y los puntos de datos más cercanos. Durante el entrenamiento, **el SVM busca el hiperplano que maximiza esta margen**. Para seleccionar los vectores de soporte, el SVM utiliza un proceso de optimización que busca minimizar una función de pérdida que penaliza la clasificación incorrecta de los puntos de datos. 

La **función kernel SVM** es una función que transforma los datos de entrada en un espacio de características de mayor dimensión, donde es más fácil encontrar un hiperplano de separación. Esto permite que el SVM maneje conjuntos de datos que no son linealmente separables en su espacio de características original. 

El **parámetro C** en SVM controla el balance entre la maximización de el margen y la clasificación incorrecta de los puntos de datos en el conjunto de entrenamiento. Un valor más bajo de C permite clasificar más puntos de datos correctamente en el conjunto de entrenamiento, pero puede conducir a un sobreajuste. Por otro lado, un valor más alto de C prioriza un margen más amplio, lo que puede resultar en una clasificación menos precisa en el conjunto de entrenamiento, pero puede generalizar mejor en datos no vistos.

El **parámetro gamma** en SVM controla el alcance de influencia de un solo ejemplo de entrenamiento. Un valor más alto de gamma significa que los puntos de datos más cercanos tienen un peso más significativo en la definición de la frontera de decisión, lo que puede conducir a un modelo más complejo y propenso al sobreajuste. Por el contrario, un valor más bajo de gamma significa que el alcance de influencia es más amplio y la frontera de decisión es más suave, sin embargo, esto podría llevar a un modelo que no generalice bien las características de los datos. 

## SVM CON TODAS LAS VARIABLES
```{r, message=FALSE, warning=FALSE}
double_cross_validation_svm <- function(data, target_name, outer, inner, kernels, costs, variables, threshold, seed) {
  set.seed(seed)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_Kernel = character(), Best_Cost = numeric(), stringsAsFactors = FALSE)

  data[[target_name]] <- as.factor(data[[target_name]])
  outer_folds <- createFolds(data[[target_name]], k = outer, list = TRUE, returnTrain = FALSE)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data[-outer_folds[[outer_index]], ]
    outer_test_data <- data[outer_folds[[outer_index]], ]

    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == "0"), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == "1"), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == "0"), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == "1"), "\n")
    
    if (sum(outer_train_data[[target_name]] == "0") == 0 || sum(outer_train_data[[target_name]] == "1") == 0) {
      cat("Skipping outer fold due to lack of class diversity\n")
      next
    }

    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner, list = TRUE, returnTrain = FALSE)
    best_model <- NULL
    best_auc <- 0
    best_kernel <- NULL
    best_cost <- NULL

    grid <- expand.grid(kernel = kernels, cost = costs)

    for (params in 1:nrow(grid)) {
      inner_auc <- numeric()

      for (inner_index in seq_along(inner_folds)) {
        inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
        inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]

        if (sum(inner_train_data[[target_name]] == "0") == 0 || sum(inner_train_data[[target_name]] == "1") == 0) {
          cat("Skipping inner fold due to lack of class diversity\n")
          next
        }

        result <- tryCatch({
          model <- svm(reformulate(variables, target_name), 
                       data = inner_train_data, kernel = grid$kernel[params], cost = grid$cost[params], 
                       probability = TRUE)

          pred <- predict(model, inner_test_data, probability = TRUE)
          prob <- attr(pred, "probabilities")[, 2]
          roc_curve <- pROC::roc(inner_test_data[[target_name]], prob)
          inner_auc[inner_index] <- roc_curve$auc
        }, error = function(e) {
          cat("Error with parameters: kernel =", grid$kernel[params], ", cost =", grid$cost[params], "\n")
          cat("Error message: ", e$message, "\n")
          inner_auc[inner_index] <- 0
        })
      }

      if (mean(inner_auc, na.rm = TRUE) > best_auc) {
        best_auc <- mean(inner_auc, na.rm = TRUE)
        best_model <- model
        best_kernel <- grid$kernel[params]
        best_cost <- grid$cost[params]
      }
    }
    best_model <- svm(reformulate(variables, target_name), 
                      data = outer_train_data, kernel = best_kernel, cost = best_cost, 
                      probability = TRUE)
    predictions <- predict(best_model, outer_test_data, probability = TRUE)
    prob <- attr(predictions, "probabilities")[, 2]
    pred_class <- ifelse(prob > threshold, 1, 0)  # Asignar clases basadas en umbral
    confusion <- table(outer_test_data[[target_name]], pred_class)
    
    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc <- pROC::roc(outer_test_data[[target_name]], prob)$auc

    # Crear un vector con los nombres correctos y convertir a dataframe
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, best_kernel, best_cost), names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
  }
  
  return(performance_metrics)
}
# Ejemplo de uso
performance_svm <- double_cross_validation_svm(data = data_factor, 
                                               outer = 5,
                                               inner = 2,
                                               target_name = "PCR",
                                               kernels = c("sigmoid", "poly", "linear"),
                                               costs = c(5, 20, 50, 100, 200, 500),
                                               variables = colnames(data_factor)[!colnames(data_factor) %in% "PCR"], seed=90, threshold=.35)

performance_svm
```
```{r}
double_cross_validation_svm <- function(data, target_name, outer, inner, kernels, costs, variables, threshold, seed) {
  set.seed(seed)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_Kernel = character(), Best_Cost = numeric(), stringsAsFactors = FALSE)

  data[[target_name]] <- as.factor(data[[target_name]])
  outer_folds <- createFolds(data[[target_name]], k = outer, list = TRUE, returnTrain = FALSE)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data[-outer_folds[[outer_index]], ]
    outer_test_data <- data[outer_folds[[outer_index]], ]

    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == "0"), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == "1"), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == "0"), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == "1"), "\n")
    
    if (sum(outer_train_data[[target_name]] == "0") == 0 || sum(outer_train_data[[target_name]] == "1") == 0) {
      cat("Skipping outer fold due to lack of class diversity\n")
      next
    }

    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner, list = TRUE, returnTrain = FALSE)
    best_model <- NULL
    best_auc <- 0
    best_kernel <- NULL
    best_cost <- NULL

    grid <- expand.grid(kernel = kernels, cost = costs)

    for (params in 1:nrow(grid)) {
      inner_auc <- numeric()

      for (inner_index in seq_along(inner_folds)) {
        inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
        inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]

        if (sum(inner_train_data[[target_name]] == "0") == 0 || sum(inner_train_data[[target_name]] == "1") == 0) {
          cat("Skipping inner fold due to lack of class diversity\n")
          next
        }

        result <- tryCatch({
          model <- svm(reformulate(variables, target_name), 
                       data = inner_train_data, kernel = grid$kernel[params], cost = grid$cost[params], 
                       probability = TRUE)

          pred <- predict(model, inner_test_data, probability = TRUE)
          prob <- attr(pred, "probabilities")[, 2]
          roc_curve <- pROC::roc(inner_test_data[[target_name]], prob)
          inner_auc[inner_index] <- roc_curve$auc
        }, error = function(e) {
          cat("Error with parameters: kernel =", grid$kernel[params], ", cost =", grid$cost[params], "\n")
          cat("Error message: ", e$message, "\n")
          inner_auc[inner_index] <- 0
        })
      }

      if (mean(inner_auc, na.rm = TRUE) > best_auc) {
        best_auc <- mean(inner_auc, na.rm = TRUE)
        best_model <- model
        best_kernel <- grid$kernel[params]
        best_cost <- grid$cost[params]
      }
    }
    
    predictions <- predict(best_model, outer_test_data, probability = TRUE)
    prob <- attr(predictions, "probabilities")[, 2]
    pred_class <- ifelse(prob > threshold, 1, 0)  # Asignar clases basadas en umbral
    confusion <- table(outer_test_data[[target_name]], pred_class)
    
    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc <- pROC::roc(outer_test_data[[target_name]], prob)$auc

    # Crear un vector con los nombres correctos y convertir a dataframe
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, best_kernel, best_cost), names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
  }
  
  return(performance_metrics)
}

performance_svm <- double_cross_validation_svm(data = data_factor, 
                                               outer = 5,
                                               inner = 2,
                                               target_name = "PCR",
                                               kernels = c("sigmoid", "poly", "linear"),
                                               costs = c(5, 20, 50, 100, 200, 500),
                                               variables = colnames(data_factor)[!colnames(data_factor) %in% "PCR"], seed=90, threshold=.35)
performance_svm
```


## SVM CON METODO WRAPPED

```{r}
library(caret)
library(e1071)
library(pROC)

double_cross_validation_svm_stepAUC <- function(data, target_name, outer, inner, kernels, costs, variables) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_Kernel = character(), Best_Cost = numeric(), Best_Variables = character(), stringsAsFactors = FALSE)
  
  data[[target_name]] <- as.factor(data[[target_name]])
  outer_folds <- createFolds(data[[target_name]], k = outer, list = TRUE, returnTrain = FALSE)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data[-outer_folds[[outer_index]], ]
    outer_test_data <- data[outer_folds[[outer_index]], ]
    
    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == 0), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == 1), "\n")
    cat("Training Data - Total: ", nrow(outer_train_data), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == 0), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == 1), "\n")
    cat("Testing Data - Total: ", nrow(outer_test_data), "\n")


    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner, list = TRUE, returnTrain = FALSE)
    best_model <- NULL
    best_auc <- 0
    best_kernel <- NULL
    best_cost <- NULL
    best_vars <- variables
    
    grid <- expand.grid(kernel = kernels, cost = costs)
    
    stepAUC <- function(vars, outer_train_data, target_name, grid, inner_folds) {
      current_vars <- vars
      best_inner_auc <- 0
      best_inner_model <- NULL
      best_inner_kernel <- NULL
      best_inner_cost <- NULL
      best_inner_vars <- current_vars
      improved <- TRUE
      
      while (improved && length(current_vars) > 1) {
        improved <- FALSE
        best_auc_in_step <- best_inner_auc
        
        for (var in current_vars) {
          temp_vars <- setdiff(current_vars, var)
          inner_auc <- numeric()
          
          for (params in 1:nrow(grid)) {
            auc_vals <- numeric()
            
            for (inner_index in seq_along(inner_folds)) {
              inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
              inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
              
              
              result <- tryCatch({
                model <- svm(as.formula(paste(target_name, "~", paste(temp_vars, collapse = "+"))),
                             data = inner_train_data, kernel = grid$kernel[params], cost = grid$cost[params], 
                             probability = TRUE)
                
                predictions <- predict(model, newdata = inner_test_data, probability = TRUE)
                predictions <- attr(predictions, "probabilities")[, 2]
                roc_curve <- roc(inner_test_data[[target_name]], predictions)
                auc_vals[inner_index] <- roc_curve$auc
              }, error = function(e) {
                cat("Error with variables:", paste(temp_vars, collapse = ", "), "\n")
                cat("Error message: ", e$message, "\n")
                auc_vals[inner_index] <- 0  # Set to NA or some other indicator of failure
              })
            }
            
            if (mean(auc_vals, na.rm = TRUE) > best_auc_in_step) {
              best_auc_in_step <- mean(auc_vals, na.rm = TRUE)
              best_inner_model <- model
              best_inner_kernel <- grid$kernel[params]
              best_inner_cost <- grid$cost[params]
              best_inner_vars <- temp_vars
              improved <- TRUE
            }
          }
        }
        
        if (improved) {
          current_vars <- best_inner_vars
          best_inner_auc <- best_auc_in_step
          cat("Vars improved: ", paste(current_vars, collapse=", "), "\n")
          cat("AUC: ", paste(best_inner_auc), "\n")
        }
      }
      
      list(model = best_inner_model, auc = best_inner_auc, 
           kernel = best_inner_kernel, cost = best_inner_cost, 
           vars = best_inner_vars)
      
    }
    best_result <- stepAUC(variables, outer_train_data, target_name, grid, inner_folds)
    best_model <- best_result$model
    best_auc <- best_result$auc
    best_kernel <- best_result$kernel
    best_cost <- best_result$cost
    best_vars <- best_result$vars
    
    predictions <- predict(best_model, newdata = outer_test_data, probability = TRUE)
    predictions <- attr(predictions, "probabilities")[, 2]
    pred_class <- ifelse(predictions > 0.35, 1, 0)  # Asignar clases basadas en umbral
    confusion <- table(Actual = outer_test_data[[target_name]], Predicted = pred_class)
    
    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc = roc(outer_test_data[[target_name]], predictions)$auc
    
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, 
                          best_kernel, best_cost, paste(best_vars, collapse = ",")), 
                        names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
    
    cat("Performance:", paste(performance_metrics))
    cat(sprintf("Confusion Matrix for Fold %d:\n", outer_index))
    print(confusion)
    cat(sprintf("Metrics for Fold %d:\n", outer_index))
    cat(sprintf("TP: %d, TN: %d, FP: %d, FN: %d, AUC: %f, Best Kernel: %s, Best Cost: %f, Best Variables: %s\n",
                TP, TN, FP, FN, auc, best_kernel, best_cost, paste(best_vars, collapse = ",")))
  }
  
  return(performance_metrics)
}

# Uso de la función con los parámetros correspondientes
performance_svm <- double_cross_validation_svm_stepAUC(data = data_numeric, 
                                                       outer = 5,
                                                       inner = 2,
                                                       target_name = "PCR",
                                                       kernels = c("linear"),
                                                       costs = c(50, 100),
                                                       variables = colnames(data_numeric)[!colnames(data_numeric) %in% "PCR"])

performance_svm

```

SVM CON WRAPPED Y VALIDACION DE FOLDS

```{r, message=FALSE, warning=FALSE}
double_cross_validation_svm_stepAUC <- function(data, target_name, outer, inner, kernels, costs, variables) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_Kernel = character(), Best_Cost = numeric(), Best_Variables = character(), stringsAsFactors = FALSE)
  
  inner_fold_metrics <- data.frame(Fold = integer(), Inner_Fold = integer(), 
                                   TP = integer(), TN = integer(), FP = integer(), 
                                   FN = integer(), AUC = numeric(), stringsAsFactors = FALSE)
  
  data[[target_name]] <- as.factor(data[[target_name]])
  outer_folds <- createFolds(data[[target_name]], k = outer, list = TRUE, returnTrain = FALSE)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data[-outer_folds[[outer_index]], ]
    outer_test_data <- data[outer_folds[[outer_index]], ]
    
    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == 0), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == 1), "\n")
    cat("Training Data - Total: ", nrow(outer_train_data), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == 0), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == 1), "\n")
    cat("Testing Data - Total: ", nrow(outer_test_data), "\n")
    
    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner, list = TRUE, returnTrain = FALSE)
    best_model <- NULL
    best_auc <- 0
    best_kernel <- NULL
    best_cost <- NULL
    best_vars <- variables
    
    grid <- expand.grid(kernel = kernels, cost = costs)
    
    stepAUC <- function(vars, outer_train_data, target_name, grid, inner_folds) {
      current_vars <- vars
      best_inner_auc <- 0
      best_inner_model <- NULL
      best_inner_kernel <- NULL
      best_inner_cost <- NULL
      best_inner_vars <- current_vars
      improved <- TRUE
      
      while (improved && length(current_vars) > 1) {
        improved <- FALSE
        best_auc_in_step <- best_inner_auc
        
        for (var in current_vars) {
          temp_vars <- setdiff(current_vars, var)
          inner_auc <- numeric()
          
          for (params in 1:nrow(grid)) {
            auc_vals <- numeric()
            
            for (inner_index in seq_along(inner_folds)) {
              inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
              inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
              
              result <- tryCatch({
                model <- svm(as.formula(paste(target_name, "~", paste(temp_vars, collapse = "+"))),
                             data = inner_train_data, kernel = grid$kernel[params], cost = grid$cost[params], 
                             probability = TRUE)
                
                predictions <- predict(model, newdata = inner_test_data, probability = TRUE)
                predictions <- attr(predictions, "probabilities")[, 2]
                roc_curve <- roc(inner_test_data[[target_name]], predictions)
                auc_vals[inner_index] <- roc_curve$auc
                
              }, error = function(e) {
                cat("Error with variables:", paste(temp_vars, collapse = ", "), "\n")
                cat("Error message: ", e$message, "\n")
                auc_vals[inner_index] <- 0  # Set to NA or some other indicator of failure
              })
            }
            
            if (mean(auc_vals, na.rm = TRUE) > best_auc_in_step) {
              best_auc_in_step <- mean(auc_vals, na.rm = TRUE)
              best_inner_model <- model
              best_inner_kernel <- grid$kernel[params]
              best_inner_cost <- grid$cost[params]
              best_inner_vars <- temp_vars
              improved <- TRUE
            }
          }
        }
        
        if (improved) {
          current_vars <- best_inner_vars
          best_inner_auc <- best_auc_in_step
          cat("Vars improved: ", paste(current_vars, collapse=", "), "\n")
          cat("AUC: ", paste(best_inner_auc), "\n")
        }
      }
      
      list(model = best_inner_model, auc = best_inner_auc, 
           kernel = best_inner_kernel, cost = best_inner_cost, 
           vars = best_inner_vars)
      
    }
    
    best_result <- stepAUC(variables, outer_train_data, target_name, grid, inner_folds)
    best_model <- best_result$model
    best_auc <- best_result$auc
    best_kernel <- best_result$kernel
    best_cost <- best_result$cost
    best_vars <- best_result$vars

    # Evaluar el mejor modelo en el validation split del inner loop
    inner_auc_vals <- numeric()
    for (inner_index in seq_along(inner_folds)) {
      inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
      inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
      
      predictions <- predict(best_model, newdata = inner_test_data, probability = TRUE)
      predictions <- attr(predictions, "probabilities")[, 2]
      roc_curve <- roc(inner_test_data[[target_name]], predictions)
      inner_auc_vals[inner_index] <- roc_curve$auc

      pred_class <- ifelse(predictions > 0.35, 1, 0)
      confusion <- table(Actual = inner_test_data[[target_name]], Predicted = pred_class)
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
      auc <- roc_curve$auc
      
      inner_metrics <- setNames(c(outer_index, inner_index, TP, TN, FP, FN, auc), names(inner_fold_metrics))
      inner_fold_metrics <- rbind(inner_fold_metrics, as.data.frame(t(inner_metrics)))
    }
    
    best_inner_auc <- mean(inner_auc_vals, na.rm = TRUE)
    
    predictions <- predict(best_model, newdata = outer_test_data, probability = TRUE)
    predictions <- attr(predictions, "probabilities")[, 2]
    pred_class <- ifelse(predictions > 0.35, 1, 0)  # Asignar clases basadas en umbral
    confusion <- table(Actual = outer_test_data[[target_name]], Predicted = pred_class)
    
    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc <- roc(outer_test_data[[target_name]], predictions)$auc
    
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, 
                          best_kernel, best_cost, paste(best_vars, collapse = ",")), 
                        names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
    
    cat("Performance:", paste(performance_metrics))
    cat(sprintf("Confusion Matrix for Fold %d:\n", outer_index))
    print(confusion)
    cat(sprintf("Metrics for Fold %d:\n", outer_index))
    cat(sprintf("TP: %d, TN: %d, FP: %d, FN: %d, AUC: %f, Best Kernel: %s, Best Cost: %f, Best Variables: %s\n",
                TP, TN, FP, FN, auc, best_kernel, best_cost, paste(best_vars, collapse = ",")))
  }
  
  list(performance_metrics = performance_metrics, inner_fold_metrics = inner_fold_metrics)
}

# Uso de la función con los parámetros correspondientes
performance_svm <- double_cross_validation_svm_stepAUC(data = data_numeric, 
                                                       outer = 5,
                                                       inner = 2,
                                                       target_name = "PCR",
                                                       kernels = c("linear"),
                                                       costs = c(100),
                                                       variables = colnames(data_numeric)[!colnames(data_numeric) %in% "PCR"])
performance_svm
```


```{r}
performance_metrics <- performance_svm$performance_metrics
inner_fold_metrics <- performance_svm$inner_fold_metrics

# Convertir las columnas a numéricas si no lo están
inner_fold_metrics <- inner_fold_metrics %>% 
  mutate(across(c(Fold, Inner_Fold, TP, TN, FP, FN, AUC), as.numeric))

performance_metrics <- performance_metrics %>% 
  mutate(across(c(Fold, TP, TN, FP, FN, AUC, Best_Cost), as.numeric))

# Calcular el AUC medio de train por cada outer fold
train_auc_means <- inner_fold_metrics %>%
  group_by(Fold) %>%
  summarise(Mean_Train_AUC = mean(AUC, na.rm = TRUE))

# Calcular el AUC medio de test por cada outer fold
test_auc_means <- performance_metrics %>%
  group_by(Fold) %>%
  summarise(Mean_Test_AUC = mean(AUC, na.rm = TRUE))

# Unir los resultados en un solo dataframe
auc_means <- train_auc_means %>%
  left_join(test_auc_means, by = "Fold") %>%
  gather(key = "Type", value = "Mean_AUC", Mean_Train_AUC, Mean_Test_AUC)

# Crear el barplot
ggplot(auc_means, aes(x = factor(Fold), y = Mean_AUC, fill = Type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "AUC medio de Train vs Test por Fold",
       x = "Outer Fold",
       y = "AUC Medio",
       fill = "Tipo de AUC") +
  theme_minimal()

ggplot(inner_fold_metrics, aes(x = factor(Fold), y = AUC)) +
  geom_boxplot() +
  labs(title = "Distribución de AUC por Fold",
       x = "Fold",
       y = "AUC") +
  theme_minimal()
```
```{r}
library(caret)
library(glmnet)
library(e1071)
library(pROC)

double_cross_validation_svm_lasso <- function(data, target_name, outer, inner, kernels, costs, variables) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_Kernel = character(), Best_Cost = numeric(),
                                    Best_Variables = character(), stringsAsFactors = FALSE)
  
  inner_fold_metrics <- data.frame(Fold = integer(), Inner_Fold = integer(), 
                                   TP = integer(), TN = integer(), FP = integer(), 
                                   FN = integer(), AUC = numeric(), stringsAsFactors = FALSE)
  
  # Convertir las variables del dataset con model.matrix
  target <- as.factor(data[[target_name]])  # Aseguramos que la variable de respuesta sea factor
  features <- data[, setdiff(names(data), target_name)]
  data_matrix <- as.data.frame(model.matrix(~ ., data=features)[,-1])  # Eliminar el intercepto
  data_matrix[[target_name]] <- target
  
  outer_folds <- createFolds(data_matrix[[target_name]], k = outer)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data_matrix[-outer_folds[[outer_index]], ]
    outer_test_data <- data_matrix[outer_folds[[outer_index]], ]
    
    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == 0), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == 1), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == 0), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == 1), "\n")
    
    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner)
    best_model <- NULL
    best_auc <- 0
    best_kernel <- NULL
    best_cost <- NULL
    best_vars <- colnames(outer_train_data)[!colnames(outer_train_data) %in% target_name]
    
    grid <- expand.grid(kernel = kernels, cost = costs)
    
    select_vars_lasso <- function(data, target_name) {
      target <- as.numeric(data[[target_name]]) - 1  # Convertimos a numérico para Lasso
      features <- data[, setdiff(names(data), target_name)]
      X <- model.matrix(~ ., data=features)[,-1]  # Eliminamos el intercepto
      y <- target
      
      # Ajustamos el modelo Lasso
      lasso_model <- cv.glmnet(X, y, alpha=1)
      lasso_coef <- coef(lasso_model, s = "lambda.min")
      
      # Seleccionamos las variables no nulas
      selected_vars <- rownames(lasso_coef)[lasso_coef[, 1] != 0]
      selected_vars <- setdiff(selected_vars, "(Intercept)")
      cat("Selected variables by Lasso: ", paste(selected_vars, collapse=", "), "\n")
      return(selected_vars)
    }
    
    stepAUC <- function(vars, data, target_name, grid, inner_folds) {
      best_inner_auc <- 0
      best_inner_model <- NULL
      best_inner_kernel <- NULL
      best_inner_cost <- NULL
      best_inner_vars <- vars
      improved <- TRUE
      
      while (improved && length(vars) > 1) {
        improved <- FALSE
        best_auc_in_step <- best_inner_auc
        
        for (params in 1:nrow(grid)) {
          auc_vals <- numeric()
          
          for (inner_index in seq_along(inner_folds)) {
            inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
            inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
            
            selected_vars <- select_vars_lasso(inner_train_data, target_name)
            formula <- as.formula(paste(target_name, "~", paste(selected_vars, collapse = "+")))
            cat("Testing with formula: ", deparse(formula), "\n")
            cat("Grid parameters - kernel: ", grid$kernel[params], ", cost: ", grid$cost[params], "\n")
            
            result <- tryCatch({
              model <- svm(formula, data = inner_train_data, kernel = grid$kernel[params], cost = grid$cost[params], probability = TRUE)
              
              predictions <- predict(model, inner_test_data, probability = TRUE)
              predictions_prob <- attr(predictions, "probabilities")[, 2]
              roc_curve <- roc(inner_test_data[[target_name]], predictions_prob)
              auc_vals[inner_index] <- roc_curve$auc
              cat("AUC for inner fold ", inner_index, ": ", roc_curve$auc, "\n")
              
            }, error = function(e) {
              cat("Error with variables: ", paste(selected_vars, collapse = ", "), "\n")
              cat("Error message: ", e$message, "\n")
              auc_vals[inner_index] <- NA  # Set to NA or some other indicator of failure
            })
          }
          
          mean_auc <- mean(auc_vals, na.rm = TRUE)
          cat("Mean AUC for params - kernel: ", grid$kernel[params], ", cost: ", grid$cost[params], ": ", mean_auc, "\n")
          if (!is.na(mean_auc) && mean_auc > best_auc_in_step) {
            best_auc_in_step <- mean_auc
            best_inner_model <- model
            best_inner_kernel <- grid$kernel[params]
            best_inner_cost <- grid$cost[params]
            best_inner_vars <- selected_vars
            improved <- TRUE
          }
        }
        
        if (improved) {
          vars <- best_inner_vars
          best_inner_auc <- best_auc_in_step
          cat("Vars improved: ", paste(vars, collapse=", "), "\n")
          cat("AUC: ", best_inner_auc, "\n")
        }
      }
      
      list(model = best_inner_model, auc = best_inner_auc, 
           kernel = best_inner_kernel, cost = best_inner_cost, 
           vars = best_inner_vars)
    }
    
    best_result <- stepAUC(best_vars, outer_train_data, target_name, grid, inner_folds)
    best_model <- best_result$model
    best_auc <- best_result$auc
    best_kernel <- best_result$kernel
    best_cost <- best_result$cost
    best_vars <- best_result$vars
    
    if (is.null(best_model)) {
      cat("Error: No valid model found for outer fold ", outer_index, "\n")
      next
    }

    # Evaluar el mejor modelo en el validation split del inner loop
    inner_auc_vals <- numeric()
    for (inner_index in seq_along(inner_folds)) {
      inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
      inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
      
      predictions <- predict(best_model, inner_test_data, probability = TRUE)
      predictions_prob <- attr(predictions, "probabilities")[, 2]
      roc_curve <- roc(inner_test_data[[target_name]], predictions_prob)
      inner_auc_vals[inner_index] <- roc_curve$auc
      cat("Inner fold ", inner_index, " AUC: ", roc_curve$auc, "\n")

      pred_class <- ifelse(predictions_prob > 0.35, "1", "0")
      confusion <- table(Actual = inner_test_data[[target_name]], Predicted = pred_class)
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
      auc <- roc_curve$auc
      
      inner_metrics <- setNames(c(outer_index, inner_index, TP, TN, FP, FN, auc), names(inner_fold_metrics))
      inner_fold_metrics <- rbind(inner_fold_metrics, as.data.frame(t(inner_metrics)))
    }
    
    best_inner_auc <- mean(inner_auc_vals, na.rm = TRUE)
    cat("Best inner AUC: ", best_inner_auc, "\n")
    
    predictions <- predict(best_model, outer_test_data, probability = TRUE)
    predictions_prob <- attr(predictions, "probabilities")[, 2]
    pred_class <- ifelse(predictions_prob > 0.35, "1", "0")  # Asignar clases basadas en umbral
    confusion <- table(Actual = outer_test_data[[target_name]], Predicted = pred_class)
    
    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc <- roc(outer_test_data[[target_name]], predictions_prob)$auc
    
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, 
                          best_kernel, best_cost, paste(best_vars, collapse = ",")), 
                        names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
    
    cat("Performance:\n", paste(performance_metrics), "\n")
    cat(sprintf("Confusion Matrix for Fold %d:\n", outer_index))
    print(confusion)
    cat(sprintf("Metrics for Fold %d:\n", outer_index))
    cat(sprintf("TP: %d, TN: %d, FP: %d, FN: %d, AUC: %f, Best Kernel: %s, Best Cost: %f, Best Variables: %s\n",
                TP, TN, FP, FN, auc, best_kernel, best_cost, paste(best_vars, collapse = ",")))
  }
  
  list(performance_metrics = performance_metrics, inner_fold_metrics = inner_fold_metrics)
}

# Uso de la función con los parámetros correspondientes
performance_svm <- double_cross_validation_svm_lasso(data = data_factor, 
                                                     outer = 5,
                                                     inner = 2,
                                                     target_name = "PCR",
                                                     kernels = c("linear"),
                                                     costs = c(100),
                                                     variables = colnames(data_factor)[!colnames(data_factor) %in% "PCR"])
performance_svm
```




## Red Neuronal con TODAS LAS VARIABLES

```{r, message=FALSE}
library(caret)
library(nnet)
library(pROC)

# Función para realizar doble validación cruzada 5x2 con nnet
double_cross_validation_nnet <- function(data, target_name, outer, inner, sizes, decays, variables) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_Size = integer(), Best_Decay = numeric(), stringsAsFactors = FALSE)
  data[[target_name]] <- as.numeric(data[[target_name]]) - 1
  outer_folds <- createFolds(data[[target_name]], k = outer, list = TRUE, returnTrain = FALSE)

  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data[-outer_folds[[outer_index]], ]
    outer_test_data <- data[outer_folds[[outer_index]], ]

    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == 0), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == 1), "\n")
    cat("Training Data - Total: ", nrow(outer_train_data), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == 0), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == 1), "\n")
    cat("Testing Data - Total: ", nrow(outer_test_data), "\n")
    
    if (sum(outer_train_data[[target_name]] == 0) == 0 || sum(outer_train_data[[target_name]] == 1) == 0) {
      cat("Skipping outer fold due to lack of class diversity\n")
      next
    }

    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner, list = TRUE, returnTrain = FALSE)
    best_model <- NULL
    best_auc <- 0
    best_size <- NULL
    best_decay <- NULL

    grid <- expand.grid(size = sizes, decay = decays)
    formula <- reformulate(variables, target_name)

    for (params in 1:nrow(grid)) {
      inner_auc <- numeric()

      for (inner_index in seq_along(inner_folds)) {
        inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
        inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]

        if (sum(inner_train_data[[target_name]] == 0) == 0 || sum(inner_train_data[[target_name]] == 1) == 0) {
          cat("Skipping inner fold due to lack of class diversity\n")
          next
        }

        result <- tryCatch({
          model <- nnet(formula, data = inner_train_data, size = grid$size[params], decay = grid$decay[params], linout = FALSE, maxit = 200)

          predictions <- predict(model, inner_test_data, type = "raw")
          roc_curve <- pROC::roc(inner_test_data[[target_name]], predictions)
          inner_auc[inner_index] <- roc_curve$auc
        }, error = function(e) {
          cat("Error with parameters: size =", grid$size[params], ", decay =", grid$decay[params], "\n")
          cat("Error message: ", e$message, "\n")
          inner_auc[inner_index] <- 0
        })
      }

      if (mean(inner_auc, na.rm = TRUE) > best_auc) {
        best_auc <- mean(inner_auc, na.rm = TRUE)
        best_model <- model
        best_size <- grid$size[params]
        best_decay <- grid$decay[params]
      }
    }

    predictions <- predict(best_model, outer_test_data, type = "raw")
    pred_class <- ifelse(predictions > 0.35, 1, 0)  # Asignar clases basadas en umbral
    confusion <- table(Actual = outer_test_data[[target_name]], Predicted = pred_class)

    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc <- pROC::roc(outer_test_data[[target_name]], predictions)$auc

    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, best_size, best_decay), names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
  }

  return(performance_metrics)
}

# Ejemplo de uso
performance_nn <- double_cross_validation_nnet(data = data_factor, 
                                               outer = 5,
                                               inner = 2,
                                               target_name = "PCR",
                                               sizes = c(5,10,15,20),
                                               decays = c(0.1, 0.2),
                                               variables = colnames(data_factor)[!colnames(data_factor) %in% "PCR"])



performance_nn

```
## Red Neuronal con Wrapped STEPAUC


```{r, message=FALSE, warning=FALSE}
double_cross_validation_nnet_stepAUC <- function(data, target_name, outer, inner, sizes, decays, variables) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_Size = integer(), Best_Decay = numeric(), Best_Variables = character(), stringsAsFactors = FALSE)
  
  data[[target_name]] <- as.numeric(data[[target_name]]) - 1
  outer_folds <- createFolds(data[[target_name]], k = outer)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data[-outer_folds[[outer_index]], ]
    outer_test_data <- data[outer_folds[[outer_index]], ]
    
    # Imprime la cantidad de instancias de cada clase en el outer train y test set
    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == 0), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == 1), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == 0), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == 1), "\n")
    
    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner)
    best_model <- NULL
    best_auc <- 0
    best_size <- NULL
    best_decay <- NULL
    best_vars <- variables
    
    grid <- expand.grid(size = sizes, decay = decays)
    
    # Implementación de "stepAUC"
    stepAUC <- function(vars, data, target_name, grid, inner_folds) {
      current_vars <- vars
      best_inner_auc <- 0
      best_inner_model <- NULL
      best_inner_size <- NULL
      best_inner_decay <- NULL
      best_inner_vars <- current_vars
      improved <- TRUE
      
      while (improved && length(current_vars) > 1) {
        improved <- FALSE
        best_auc_in_step <- best_inner_auc
        
        for (var in current_vars) {
          temp_vars <- setdiff(current_vars, var)
          inner_auc <- numeric()
          
          for (params in 1:nrow(grid)) {
            auc_vals <- numeric()
            
            for (inner_index in seq_along(inner_folds)) {
              inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
              inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
              
              result <- tryCatch({
                model <- nnet(as.formula(paste(target_name, "~", paste(temp_vars, collapse = "+"))),
                              data = inner_train_data, size = grid$size[params], decay = grid$decay[params], 
                              linout = FALSE, maxit = 200, trace = FALSE)
                
                predictions <- predict(model, inner_test_data, type = "raw")
                roc_curve <- roc(inner_test_data[[target_name]], predictions)
                auc_vals[inner_index] <- roc_curve$auc
              }, error = function(e) {
                cat("Error with variables:", paste(temp_vars, collapse = ", "), "\n")
                auc_vals[inner_index] <- 0  # Set to NA or some other indicator of failure
              })
            }
            
            if (mean(auc_vals, na.rm = TRUE) > best_auc_in_step) {
              best_auc_in_step <- mean(auc_vals, na.rm = TRUE)
              best_inner_model <- model
              best_inner_size <- grid$size[params]
              best_inner_decay <- grid$decay[params]
              best_inner_vars <- temp_vars
              improved <- TRUE
            }
          }
        }
        
        if (improved) {
          current_vars <- best_inner_vars
          best_inner_auc <- best_auc_in_step
          cat("Vars improved: ", paste(current_vars, collapse=", "), "\n")
          cat("AUC: ", paste(best_inner_auc), "\n")
        }
      }
      
      list(model = best_inner_model, auc = best_inner_auc, 
           size = best_inner_size, decay = best_inner_decay, 
           vars = best_inner_vars)
      
    }
    best_result <- stepAUC(variables, outer_train_data, target_name, grid, inner_folds)
    best_model <- best_result$model
    best_auc <- best_result$auc
    best_size <- best_result$size
    best_decay <- best_result$decay
    best_vars <- best_result$vars
    
    predictions <- predict(best_model, outer_test_data, type = "raw")
    pred_class <- ifelse(predictions > 0.35, 1, 0)  # Asignar clases basadas en umbral
    confusion <- table(Actual = outer_test_data[[target_name]], Predicted = pred_class)
    
    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc = roc(outer_test_data[[target_name]], predictions)$auc
    
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, 
                          best_size, best_decay, paste(best_vars, collapse = ",")), 
                        names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
    
    cat("Performance:", paste(performance_metrics))
    cat(sprintf("Confusion Matrix for Fold %d:\n", outer_index))
    print(confusion)
    cat(sprintf("Metrics for Fold %d:\n", outer_index))
    cat(sprintf("TP: %d, TN: %d, FP: %d, FN: %d, AUC: %f, Best Size: %d, Best Decay: %f, Best Variables: %s\n",
                TP, TN, FP, FN, auc, best_size, best_decay, paste(best_vars, collapse = ",")))
  }
  
  return(performance_metrics)
}

# Uso de la función con los parámetros correspondientes
performance_nn <- double_cross_validation_nnet_stepAUC(data = data_factor, 
                                                       outer = 5,
                                                       inner = 2,
                                                       target_name = "PCR",
                                                       sizes = c(5, 10, 15, 20),
                                                       decays = c(0.1, 0.2),
                                                       variables = colnames(data)[!colnames(data) %in% "PCR"])

performance_nn
```




```{r}
double_cross_validation_nnet_stepAUC <- function(data, target_name, outer, inner, sizes, decays, variables) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_Size = integer(), Best_Decay = numeric(), Best_Variables = character(), stringsAsFactors = FALSE)
  
  inner_fold_metrics <- data.frame(Fold = integer(), Inner_Fold = integer(), 
                                   TP = integer(), TN = integer(), FP = integer(), 
                                   FN = integer(), AUC = numeric(), stringsAsFactors = FALSE)
  
  data[[target_name]] <- as.numeric(data[[target_name]]) - 1
  outer_folds <- createFolds(data[[target_name]], k = outer)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data[-outer_folds[[outer_index]], ]
    outer_test_data <- data[outer_folds[[outer_index]], ]
    
    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == 0), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == 1), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == 0), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == 1), "\n")
    
    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner)
    best_model <- NULL
    best_auc <- 0
    best_size <- NULL
    best_decay <- NULL
    best_vars <- variables
    
    grid <- expand.grid(size = sizes, decay = decays)
    
    stepAUC <- function(vars, data, target_name, grid, inner_folds) {
      current_vars <- vars
      best_inner_auc <- 0
      best_inner_model <- NULL
      best_inner_size <- NULL
      best_inner_decay <- NULL
      best_inner_vars <- current_vars
      improved <- TRUE
      
      while (improved && length(current_vars) > 1) {
        improved <- FALSE
        best_auc_in_step <- best_inner_auc
        
        for (var in current_vars) {
          temp_vars <- setdiff(current_vars, var)
          inner_auc <- numeric()
          
          for (params in 1:nrow(grid)) {
            auc_vals <- numeric()
            
            for (inner_index in seq_along(inner_folds)) {
              inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
              inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
              
              result <- tryCatch({
                model <- nnet(as.formula(paste(target_name, "~", paste(temp_vars, collapse = "+"))),
                              data = inner_train_data, size = grid$size[params], decay = grid$decay[params], 
                              linout = FALSE, maxit = 200, trace = FALSE)
                
                predictions <- predict(model, inner_test_data, type = "raw")
                roc_curve <- roc(inner_test_data[[target_name]], predictions)
                auc_vals[inner_index] <- roc_curve$auc
                
                # Guardar los datos del inner fold
                pred_class <- ifelse(predictions > 0.35, 1, 0)
                confusion <- table(Actual = inner_test_data[[target_name]], Predicted = pred_class)
                TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
                TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
                FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
                FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
                auc <- roc_curve$auc
                inner_metrics <- setNames(c(outer_index, inner_index, TP, TN, FP, FN, auc), names(inner_fold_metrics))
                inner_fold_metrics <<- rbind(inner_fold_metrics, as.data.frame(t(inner_metrics)))
              }, error = function(e) {
                cat("Error with variables:", paste(temp_vars, collapse = ", "), "\n")
                auc_vals[inner_index] <- 0  # Set to NA or some other indicator of failure
              })
            }
            
            if (mean(auc_vals, na.rm = TRUE) > best_auc_in_step) {
              best_auc_in_step <- mean(auc_vals, na.rm = TRUE)
              best_inner_model <- model
              best_inner_size <- grid$size[params]
              best_inner_decay <- grid$decay[params]
              best_inner_vars <- temp_vars
              improved <- TRUE
            }
          }
        }
        
        if (improved) {
          current_vars <- best_inner_vars
          best_inner_auc <- best_auc_in_step
          cat("Vars improved: ", paste(current_vars, collapse=", "), "\n")
          cat("AUC: ", paste(best_inner_auc), "\n")
        }
      }
      
      list(model = best_inner_model, auc = best_inner_auc, 
           size = best_inner_size, decay = best_inner_decay, 
           vars = best_inner_vars)
    }
    
    best_result <- stepAUC(variables, outer_train_data, target_name, grid, inner_folds)
    best_model <- best_result$model
    best_auc <- best_result$auc
    best_size <- best_result$size
    best_decay <- best_result$decay
    best_vars <- best_result$vars
    
    predictions <- predict(best_model, outer_test_data, type = "raw")
    pred_class <- ifelse(predictions > 0.35, 1, 0)  # Asignar clases basadas en umbral
    confusion <- table(Actual = outer_test_data[[target_name]], Predicted = pred_class)
    
    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc = roc(outer_test_data[[target_name]], predictions)$auc
    
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, 
                          best_size, best_decay, paste(best_vars, collapse = ",")), 
                        names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
    
    cat("Performance:", paste(performance_metrics))
    cat(sprintf("Confusion Matrix for Fold %d:\n", outer_index))
    print(confusion)
    cat(sprintf("Metrics for Fold %d:\n", outer_index))
    cat(sprintf("TP: %d, TN: %d, FP: %d, FN: %d, AUC: %f, Best Size: %d, Best Decay: %f, Best Variables: %s\n",
                TP, TN, FP, FN, auc, best_size, best_decay, paste(best_vars, collapse = ",")))
  }
  
  list(performance_metrics = performance_metrics, inner_fold_metrics = inner_fold_metrics)
}

# Uso de la función con los parámetros correspondientes
performance_nn <- double_cross_validation_nnet_stepAUC(data = data_factor, 
                                                       outer = 5,
                                                       inner = 2,
                                                       target_name = "PCR",
                                                       sizes = c(5, 10, 15, 20),
                                                       decays = c(0.1, 0.2),
                                                       variables = colnames(data_factor)[!colnames(data_factor) %in% "PCR"])

performance_nn$performance_metrics
performance_nn$inner_fold_metrics
```






```{r, message=FALSE, warning=FALSE}
double_cross_validation_nnet_stepAUC <- function(data, target_name, outer, inner, sizes, decays, variables) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_Size = integer(), Best_Decay = numeric(), Best_Variables = character(), stringsAsFactors = FALSE)
  
  inner_fold_metrics <- data.frame(Fold = integer(), Inner_Fold = integer(), 
                                   TP = integer(), TN = integer(), FP = integer(), 
                                   FN = integer(), AUC = numeric(), stringsAsFactors = FALSE)
  
  data[[target_name]] <- as.numeric(data[[target_name]]) - 1
  outer_folds <- createFolds(data[[target_name]], k = outer)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data[-outer_folds[[outer_index]], ]
    outer_test_data <- data[outer_folds[[outer_index]], ]
    
    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == 0), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == 1), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == 0), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == 1), "\n")
    
    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner)
    best_model <- NULL
    best_auc <- 0
    best_size <- NULL
    best_decay <- NULL
    best_vars <- variables
    
    grid <- expand.grid(size = sizes, decay = decays)
    
    stepAUC <- function(vars, data, target_name, grid, inner_folds) {
      current_vars <- vars
      best_inner_auc <- 0
      best_inner_model <- NULL
      best_inner_size <- NULL
      best_inner_decay <- NULL
      best_inner_vars <- current_vars
      improved <- TRUE
      
      while (improved && length(current_vars) > 1) {
        improved <- FALSE
        best_auc_in_step <- best_inner_auc
        
        for (var in current_vars) {
          temp_vars <- setdiff(current_vars, var)
          inner_auc <- numeric()
          
          for (params in 1:nrow(grid)) {
            auc_vals <- numeric()
            
            for (inner_index in seq_along(inner_folds)) {
              inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
              inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
              
              result <- tryCatch({
                model <- nnet(as.formula(paste(target_name, "~", paste(temp_vars, collapse = "+"))),
                              data = inner_train_data, size = grid$size[params], decay = grid$decay[params], 
                              linout = FALSE, maxit = 200, trace = FALSE)
                
                predictions <- predict(model, inner_test_data, type = "raw")
                roc_curve <- roc(inner_test_data[[target_name]], predictions)
                auc_vals[inner_index] <- roc_curve$auc
                
              }, error = function(e) {
                cat("Error with variables:", paste(temp_vars, collapse = ", "), "\n")
                auc_vals[inner_index] <- 0  # Set to NA or some other indicator of failure
              })
            }
            
            if (mean(auc_vals, na.rm = TRUE) > best_auc_in_step) {
              best_auc_in_step <- mean(auc_vals, na.rm = TRUE)
              best_inner_model <- model
              best_inner_size <- grid$size[params]
              best_inner_decay <- grid$decay[params]
              best_inner_vars <- temp_vars
              improved <- TRUE
            }
          }
        }
        
        if (improved) {
          current_vars <- best_inner_vars
          best_inner_auc <- best_auc_in_step
          cat("Vars improved: ", paste(current_vars, collapse=", "), "\n")
          cat("AUC: ", paste(best_inner_auc), "\n")
        }
      }
      
      list(model = best_inner_model, auc = best_inner_auc, 
           size = best_inner_size, decay = best_inner_decay, 
           vars = best_inner_vars)
    }
    
    best_result <- stepAUC(variables, outer_train_data, target_name, grid, inner_folds)
    best_model <- best_result$model
    best_auc <- best_result$auc
    best_size <- best_result$size
    best_decay <- best_result$decay
    best_vars <- best_result$vars

    # Evaluar el mejor modelo en el validation split del inner loop
    inner_auc_vals <- numeric()
    for (inner_index in seq_along(inner_folds)) {
      inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
      inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
      
      predictions <- predict(best_model, inner_test_data, type = "raw")
      roc_curve <- roc(inner_test_data[[target_name]], predictions)
      inner_auc_vals[inner_index] <- roc_curve$auc

      pred_class <- ifelse(predictions > 0.35, 1, 0)
      confusion <- table(Actual = inner_test_data[[target_name]], Predicted = pred_class)
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
      auc <- roc_curve$auc
      
      inner_metrics <- setNames(c(outer_index, inner_index, TP, TN, FP, FN, auc), names(inner_fold_metrics))
      inner_fold_metrics <- rbind(inner_fold_metrics, as.data.frame(t(inner_metrics)))
    }
    
    best_inner_auc <- mean(inner_auc_vals, na.rm = TRUE)
    
    predictions <- predict(best_model, outer_test_data, type = "raw")
    pred_class <- ifelse(predictions > 0.35, 1, 0)  # Asignar clases basadas en umbral
    confusion <- table(Actual = outer_test_data[[target_name]], Predicted = pred_class)
    
    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc <- roc(outer_test_data[[target_name]], predictions)$auc
    
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, 
                          best_size, best_decay, paste(best_vars, collapse = ",")), 
                        names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
    
    cat("Performance:", paste(performance_metrics))
    cat(sprintf("Confusion Matrix for Fold %d:\n", outer_index))
    print(confusion)
    cat(sprintf("Metrics for Fold %d:\n", outer_index))
    cat(sprintf("TP: %d, TN: %d, FP: %d, FN: %d, AUC: %f, Best Size: %d, Best Decay: %f, Best Variables: %s\n",
                TP, TN, FP, FN, auc, best_size, best_decay, paste(best_vars, collapse = ",")))
  }
  
  list(performance_metrics = performance_metrics, inner_fold_metrics = inner_fold_metrics)
}

# Uso de la función con los parámetros correspondientes
performance_nn <- double_cross_validation_nnet_stepAUC(data = data_factor, 
                                                       outer = 5,
                                                       inner = 2,
                                                       target_name = "PCR",
                                                       sizes = c(5, 10, 15, 20),
                                                       decays = c(0.1, 0.2),
                                                       variables = colnames(data)[!colnames(data) %in% "PCR"])
performance_nn
```


```{r}
performance_metrics <- performance_nn$performance_metrics
inner_fold_metrics <- performance_nn$inner_fold_metrics

# Convertir las columnas a numéricas si no lo están
inner_fold_metrics <- inner_fold_metrics %>% 
  mutate(across(c(Fold, Inner_Fold, TP, TN, FP, FN, AUC), as.numeric))

performance_metrics <- performance_metrics %>% 
  mutate(across(c(Fold, TP, TN, FP, FN, AUC, Best_Size, Best_Decay), as.numeric))

# Calcular el AUC medio de train por cada outer fold
train_auc_means <- inner_fold_metrics %>%
  group_by(Fold) %>%
  summarise(Mean_Train_AUC = mean(AUC, na.rm = TRUE))

# Calcular el AUC medio de test por cada outer fold
test_auc_means <- performance_metrics %>%
  group_by(Fold) %>%
  summarise(Mean_Test_AUC = mean(AUC, na.rm = TRUE))

# Unir los resultados en un solo dataframe
auc_means <- train_auc_means %>%
  left_join(test_auc_means, by = "Fold") %>%
  gather(key = "Type", value = "Mean_AUC", Mean_Train_AUC, Mean_Test_AUC)

# Crear el barplot
ggplot(auc_means, aes(x = factor(Fold), y = Mean_AUC, fill = Type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "AUC medio de Train vs Test por Fold",
       x = "Outer Fold",
       y = "AUC Medio",
       fill = "Tipo de AUC") +
  theme_minimal()

ggplot(inner_fold_metrics, aes(x = factor(Fold), y = AUC)) +
  geom_boxplot() +
  labs(title = "Distribución de AUC por Fold",
       x = "Fold",
       y = "AUC") +
  theme_minimal()
```

## Método embedded nnet

```{r, message=FALSE, warning=FALSE}

target <- as.numeric(data_factor$PCR)
features <- data_factor[, setdiff(names(data_factor), "PCR")]

X <- model.matrix(~ ., data=features)[,-1]  # Eliminar el intercepto
y <- target

lasso_model <- cv.glmnet(X, y, alpha=1)  # alpha = 1 para Lasso
print(lasso_model)

# Coeficientes del modelo Lasso
lasso_coef <- coef(lasso_model, s = "lambda.min")
print(lasso_coef)

# Ajustamos el modelo Ridge (L2 regularización)
ridge_model <- cv.glmnet(X, y, alpha=0)  # alpha = 0 para Ridge
print(ridge_model)

# Coeficientes del modelo Ridge
ridge_coef <- coef(ridge_model, s = "lambda.min")
print(ridge_coef)
```

```{r}
# Supongamos que 'data_numeric' es tu dataset y la columna de la variable objetivo se llama 'PCR'
target <- data_numeric$PCR
features <- data_numeric[, setdiff(names(data_numeric), "PCR")]
# Definimos la matriz de características y la variable objetivo
X <- as.matrix(features)  # Convertimos las características a una matriz
y <- target  # La variable objetivo ya es numérica

# Ajustamos el modelo Lasso (L1 regularización)
lasso_model <- cv.glmnet(X, y, alpha=1)  # alpha = 1 para Lasso
print(lasso_model)

# Ajustamos el modelo Ridge (L2 regularización)
ridge_model <- cv.glmnet(X, y, alpha=0)  # alpha = 0 para Ridge
print(ridge_model)

# Coeficientes del modelo Lasso
lasso_coef <- coef(lasso_model, s = "lambda.min")
print(lasso_coef)

# Coeficientes del modelo Ridge
ridge_coef <- coef(ridge_model, s = "lambda.min")
print(ridge_coef)

```

```{r}
library(glmnet)
library(caret)
library(nnet)
library(pROC)
library(dplyr)

double_cross_validation_nnet_lasso <- function(data, target_name, outer, inner, sizes, decays) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_Size = integer(), Best_Decay = numeric(), Best_Variables = character(), stringsAsFactors = FALSE)
  
  inner_fold_metrics <- data.frame(Fold = integer(), Inner_Fold = integer(), 
                                   TP = integer(), TN = integer(), FP = integer(), 
                                   FN = integer(), AUC = numeric(), stringsAsFactors = FALSE)
  
  # Convertir las variables del dataset con model.matrix
  target <- as.numeric(data[[target_name]]) - 1
  features <- data[, setdiff(names(data), target_name)]
  data_matrix <- as.data.frame(model.matrix(~ ., data=features)[,-1])  # Eliminar el intercepto
  data_matrix[[target_name]] <- target
  
  outer_folds <- createFolds(data_matrix[[target_name]], k = outer)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data_matrix[-outer_folds[[outer_index]], ]
    outer_test_data <- data_matrix[outer_folds[[outer_index]], ]
    
    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == 0), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == 1), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == 0), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == 1), "\n")
    
    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner)
    best_model <- NULL
    best_auc <- 0
    best_size <- NULL
    best_decay <- NULL
    best_vars <- colnames(outer_train_data)[!colnames(outer_train_data) %in% target_name]
    
    grid <- expand.grid(size = sizes, decay = decays)
    
    select_vars_lasso <- function(data, target_name) {
      target <- as.numeric(data[[target_name]])
      features <- data[, setdiff(names(data), target_name)]
      X <- model.matrix(~ ., data=features)[,-1]  # Eliminamos el intercepto
      y <- target
      
      # Ajustamos el modelo Lasso
      lasso_model <- cv.glmnet(X, y, alpha=1)
      lasso_coef <- coef(lasso_model, s = "lambda.min")
      
      # Seleccionamos las variables no nulas
      selected_vars <- rownames(lasso_coef)[lasso_coef[, 1] != 0]
      selected_vars <- setdiff(selected_vars, "(Intercept)")
      cat("Selected variables by Lasso: ", paste(selected_vars, collapse=", "), "\n")
      return(selected_vars)
    }
    
    stepAUC <- function(vars, data, target_name, grid, inner_folds) {
      best_inner_auc <- 0
      best_inner_model <- NULL
      best_inner_size <- NULL
      best_inner_decay <- NULL
      best_inner_vars <- vars
      improved <- TRUE
      
      while (improved && length(vars) > 1) {
        improved <- FALSE
        best_auc_in_step <- best_inner_auc
        
        for (params in 1:nrow(grid)) {
          auc_vals <- numeric()
          
          for (inner_index in seq_along(inner_folds)) {
            inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
            inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
            
            selected_vars <- select_vars_lasso(inner_train_data, target_name)
            formula <- as.formula(paste(target_name, "~", paste(selected_vars, collapse = "+")))
            cat("Testing with formula: ", deparse(formula), "\n")
            cat("Grid parameters - size: ", grid$size[params], ", decay: ", grid$decay[params], "\n")
            
            result <- tryCatch({
              model <- nnet(formula, data = inner_train_data, size = grid$size[params], 
                            decay = grid$decay[params], linout = FALSE, maxit = 200, trace = FALSE)
              
              predictions <- predict(model, inner_test_data, type = "raw")
              roc_curve <- roc(inner_test_data[[target_name]], predictions)
              auc_vals[inner_index] <- roc_curve$auc
              cat("AUC for inner fold ", inner_index, ": ", roc_curve$auc, "\n")
              
            }, error = function(e) {
              cat("Error with variables: ", paste(selected_vars, collapse = ", "), "\n")
              cat("Error message: ", e$message, "\n")
              auc_vals[inner_index] <- NA  # Set to NA or some other indicator of failure
            })
          }
          
          mean_auc <- mean(auc_vals, na.rm = TRUE)
          cat("Mean AUC for params - size: ", grid$size[params], ", decay: ", grid$decay[params], ": ", mean_auc, "\n")
          if (!is.na(mean_auc) && mean_auc > best_auc_in_step) {
            best_auc_in_step <- mean_auc
            best_inner_model <- model
            best_inner_size <- grid$size[params]
            best_inner_decay <- grid$decay[params]
            best_inner_vars <- selected_vars
            improved <- TRUE
          }
        }
        
        if (improved) {
          vars <- best_inner_vars
          best_inner_auc <- best_auc_in_step
          cat("Vars improved: ", paste(vars, collapse=", "), "\n")
          cat("AUC: ", best_inner_auc, "\n")
        }
      }
      
      list(model = best_inner_model, auc = best_inner_auc, 
           size = best_inner_size, decay = best_inner_decay, 
           vars = best_inner_vars)
    }
    
    best_result <- stepAUC(best_vars, outer_train_data, target_name, grid, inner_folds)
    best_model <- best_result$model
    best_auc <- best_result$auc
    best_size <- best_result$size
    best_decay <- best_result$decay
    best_vars <- best_result$vars
    
    if (is.null(best_model)) {
      cat("Error: No valid model found for outer fold ", outer_index, "\n")
      next
    }

    # Evaluar el mejor modelo en el validation split del inner loop
    inner_auc_vals <- numeric()
    for (inner_index in seq_along(inner_folds)) {
      inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
      inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
      
      predictions <- predict(best_model, inner_test_data, type = "raw")
      roc_curve <- roc(inner_test_data[[target_name]], predictions)
      inner_auc_vals[inner_index] <- roc_curve$auc
      cat("Inner fold ", inner_index, " AUC: ", roc_curve$auc, "\n")

      pred_class <- ifelse(predictions > 0.35, 1, 0)
      confusion <- table(Actual = inner_test_data[[target_name]], Predicted = pred_class)
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
      auc <- roc_curve$auc
      
      inner_metrics <- setNames(c(outer_index, inner_index, TP, TN, FP, FN, auc), names(inner_fold_metrics))
      inner_fold_metrics <- rbind(inner_fold_metrics, as.data.frame(t(inner_metrics)))
    }
    
    best_inner_auc <- mean(inner_auc_vals, na.rm = TRUE)
    cat("Best inner AUC: ", best_inner_auc, "\n")
    
    predictions <- predict(best_model, outer_test_data, type = "raw")
    pred_class <- ifelse(predictions > 0.35, 1, 0)  # Asignar clases basadas en umbral
    confusion <- table(Actual = outer_test_data[[target_name]], Predicted = pred_class)
    
    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc <- roc(outer_test_data[[target_name]], predictions)$auc
    
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, 
                          best_size, best_decay, paste(best_vars, collapse = ",")), 
                        names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
    
    cat("Performance:\n", paste(performance_metrics), "\n")
    cat(sprintf("Confusion Matrix for Fold %d:\n", outer_index))
    print(confusion)
    cat(sprintf("Metrics for Fold %d:\n", outer_index))
    cat(sprintf("TP: %d, TN: %d, FP: %d, FN: %d, AUC: %f, Best Size: %d, Best Decay: %f, Best Variables: %s\n",
                TP, TN, FP, FN, auc, best_size, best_decay, paste(best_vars, collapse = ",")))
  }
  
  list(performance_metrics = performance_metrics, inner_fold_metrics = inner_fold_metrics)
}

# Uso de la función con los parámetros correspondientes
performance_nn <- double_cross_validation_nnet_lasso(data = data_factor, 
                                                     outer = 5,
                                                     inner = 2,
                                                     target_name = "PCR",
                                                     sizes = c(5, 10, 15, 20),
                                                     decays = c(0.1, 0.2))
performance_nn

data_factor


```


## TODAS LAS VARIABLES RPART


```{r, message=FALSE, warning=FALSE}

double_cross_validation_rpart <- function(data, target_name, outer, inner, cp_values, minsplit_values, variables) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_cp = numeric(), Best_minsplit = integer(), stringsAsFactors = FALSE)
  
  data[[target_name]] <- as.factor(data[[target_name]])
  outer_folds <- createFolds(data[[target_name]], k = outer)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data[-outer_folds[[outer_index]], ]
    outer_test_data <- data[outer_folds[[outer_index]], ]
    
    # Imprime la cantidad de instancias de cada clase en el outer train y test set
    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == 0), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == 1), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == 0), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == 1), "\n")
    
    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner)
    best_model <- NULL
    best_auc <- 0
    best_cp <- NULL
    best_minsplit <- NULL
    
    grid <- expand.grid(cp = cp_values, minsplit = minsplit_values)
    
    for (params in 1:nrow(grid)) {
      inner_auc <- numeric()
      
      for (inner_index in seq_along(inner_folds)) {
        inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
        inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
        
        model <- rpart(formula = as.formula(paste(target_name, "~", paste(variables, collapse = "+"))),
                       data = inner_train_data, control = rpart.control(cp = grid$cp[params], minsplit = grid$minsplit[params]))
        
        predictions <- predict(model, inner_test_data, type = "prob")[, 2]
        roc_curve <- roc(inner_test_data[[target_name]], predictions)
        inner_auc[inner_index] <- roc_curve$auc
      }
      
      if (mean(inner_auc) > best_auc) {
        best_auc = mean(inner_auc)
        best_model = model
        best_cp = grid$cp[params]
        best_minsplit = grid$minsplit[params]
      }
    }
    
    predictions <- predict(best_model, outer_test_data, type = "prob")[, 2]
    pred_class <- ifelse(predictions > 0.35, 1, 0)  # Asignar clases basadas en umbral
    confusion <- table(Actual = outer_test_data[[target_name]], Predicted = pred_class)
    
    TP = confusion[2, 2]
    TN = confusion[1, 1]
    FP = confusion[1, 2]
    FN = confusion[2, 1]
    auc = roc(outer_test_data[[target_name]], predictions)$auc
    
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, best_cp, best_minsplit), names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
  }
  
  return(performance_metrics)
}

# Uso de la función con los parámetros correspondientes
performance_rpart <- double_cross_validation_rpart(data = data,
                                                   outer = 5,
                                                   inner = 2,
                                                   target_name = "PCR",
                                                   cp_values = c(0.001, 0.01, 0.1),
                                                   minsplit_values = c(2, 5, 10, 15, 20),
                                                   variables = colnames(data)[!colnames(data) %in% "PCR"])

performance_rpart
```

## RPART CON WRAPPED Y VALIDACION DE FOLD INTERNOS

```{r, message=FALSE, warning=FALSE}
double_cross_validation_rpart_stepAUC <- function(data, target_name, outer, inner, cps, minsplits, variables) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_CP = numeric(), Best_Minsplit = integer(), 
                                    Best_Variables = character(), stringsAsFactors = FALSE)
  
  inner_fold_metrics <- data.frame(Fold = integer(), Inner_Fold = integer(), 
                                   TP = integer(), TN = integer(), FP = integer(), 
                                   FN = integer(), AUC = numeric(), stringsAsFactors = FALSE)
  
  data[[target_name]] <- as.factor(data[[target_name]])
  outer_folds <- createFolds(data[[target_name]], k = outer)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data[-outer_folds[[outer_index]], ]
    outer_test_data <- data[outer_folds[[outer_index]], ]
    
    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == "0"), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == "1"), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == "0"), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == "1"), "\n")
    
    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner)
    best_model <- NULL
    best_auc <- 0
    best_cp <- NULL
    best_minsplit <- NULL
    best_vars <- variables
    
    grid <- expand.grid(cp = cps, minsplit = minsplits)
    
    stepAUC <- function(vars, data, target_name, grid, inner_folds) {
      current_vars <- vars
      best_inner_auc <- 0
      best_inner_model <- NULL
      best_inner_cp <- NULL
      best_inner_minsplit <- NULL
      best_inner_vars <- current_vars
      improved <- TRUE
      
      while (improved && length(current_vars) > 1) {
        improved <- FALSE
        best_auc_in_step <- best_inner_auc
        
        for (var in current_vars) {
          temp_vars <- setdiff(current_vars, var)
          inner_auc <- numeric()
          
          for (params in 1:nrow(grid)) {
            auc_vals <- numeric()
            
            for (inner_index in seq_along(inner_folds)) {
              inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
              inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
              
              result <- tryCatch({
                model <- rpart(as.formula(paste(target_name, "~", paste(temp_vars, collapse = "+"))),
                               data = inner_train_data, control = rpart.control(cp = grid$cp[params], minsplit = grid$minsplit[params]))
                
                predictions <- predict(model, inner_test_data, type = "prob")[, 2]
                roc_curve <- roc(inner_test_data[[target_name]], predictions)
                auc_vals[inner_index] <- roc_curve$auc
                
              }, error = function(e) {
                cat("Error with variables:", paste(temp_vars, collapse = ", "), "\n")
                auc_vals[inner_index] <- 0
              })
            }
            
            if (mean(auc_vals, na.rm = TRUE) > best_auc_in_step) {
              best_auc_in_step <- mean(auc_vals, na.rm = TRUE)
              best_inner_model <- model
              best_inner_cp <- grid$cp[params]
              best_inner_minsplit <- grid$minsplit[params]
              best_inner_vars <- temp_vars
              improved <- TRUE
            }
          }
        }
        
        if (improved) {
          current_vars <- best_inner_vars
          best_inner_auc <- best_auc_in_step
          cat("Vars improved: ", paste(current_vars, collapse=", "), "\n")
          cat("AUC: ", paste(best_inner_auc), "\n")
        }
      }
      
      list(model = best_inner_model, auc = best_inner_auc, 
           cp = best_inner_cp, minsplit = best_inner_minsplit, 
           vars = best_inner_vars)
    }
    
    best_result <- stepAUC(variables, outer_train_data, target_name, grid, inner_folds)
    best_model <- best_result$model
    best_auc <- best_result$auc
    best_cp <- best_result$cp
    best_minsplit <- best_result$minsplit
    best_vars <- best_result$vars

    # Evaluar el mejor modelo en el validation split del inner loop
    inner_auc_vals <- numeric()
    for (inner_index in seq_along(inner_folds)) {
      inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
      inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
      
      predictions <- predict(best_model, inner_test_data, type = "prob")[, 2]
      roc_curve <- roc(inner_test_data[[target_name]], predictions)
      inner_auc_vals[inner_index] <- roc_curve$auc

      pred_class <- ifelse(predictions > 0.35, "1", "0")
      confusion <- table(Actual = inner_test_data[[target_name]], Predicted = pred_class)
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
      auc <- roc_curve$auc
      
      inner_metrics <- setNames(c(outer_index, inner_index, TP, TN, FP, FN, auc), names(inner_fold_metrics))
      inner_fold_metrics <- rbind(inner_fold_metrics, as.data.frame(t(inner_metrics)))
    }
    
    best_inner_auc <- mean(inner_auc_vals, na.rm = TRUE)
    
    predictions <- predict(best_model, outer_test_data, type = "prob")[, 2]
    pred_class <- ifelse(predictions > 0.35, "1", "0")
    confusion <- table(Actual = outer_test_data[[target_name]], Predicted = pred_class)
    
    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc <- roc(outer_test_data[[target_name]], predictions)$auc
    
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, 
                          best_cp, best_minsplit, paste(best_vars, collapse = ",")), 
                        names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
    
    cat("Performance:", paste(performance_metrics))
    cat(sprintf("Confusion Matrix for Fold %d:\n", outer_index))
    print(confusion)
    cat(sprintf("Metrics for Fold %d:\n", outer_index))
    cat(sprintf("TP: %d, TN: %d, FP: %d, FN: %d, AUC: %f, Best CP: %f, Best Minsplit: %d, Best Variables: %s\n",
                TP, TN, FP, FN, auc, best_cp, best_minsplit, paste(best_vars, collapse = ",")))
  }
  
  list(performance_metrics = performance_metrics, inner_fold_metrics = inner_fold_metrics)
}

# Uso de la función con los parámetros correspondientes
performance_rf <- double_cross_validation_rpart_stepAUC(data = data_factor, 
                                                        outer = 5,
                                                        inner = 2,
                                                        target_name = "PCR",
                                                        cps = c(0.01, 0.05, 0.1),
                                                        minsplits = c(5, 10, 15),
                                                        variables = colnames(data)[!colnames(data) %in% "PCR"])
performance_rf
```



```{r}
performance_metrics <- performance_rf$performance_metrics
inner_fold_metrics <- performance_rf$inner_fold_metrics

# Convertir las columnas a numéricas si no lo están
inner_fold_metrics <- inner_fold_metrics %>% 
  mutate(across(c(Fold, Inner_Fold, TP, TN, FP, FN, AUC), as.numeric))

performance_metrics <- performance_metrics %>% 
  mutate(across(c(Fold, TP, TN, FP, FN, AUC, Best_CP, Best_Minsplit), as.numeric))

# Calcular el AUC medio de train por cada outer fold
train_auc_means <- inner_fold_metrics %>%
  group_by(Fold) %>%
  summarise(Mean_Train_AUC = mean(AUC, na.rm = TRUE))

# Calcular el AUC medio de test por cada outer fold
test_auc_means <- performance_metrics %>%
  group_by(Fold) %>%
  summarise(Mean_Test_AUC = mean(AUC, na.rm = TRUE))

# Unir los resultados en un solo dataframe
auc_means <- train_auc_means %>%
  left_join(test_auc_means, by = "Fold") %>%
  gather(key = "Type", value = "Mean_AUC", Mean_Train_AUC, Mean_Test_AUC)

# Crear el barplot
ggplot(auc_means, aes(x = factor(Fold), y = Mean_AUC, fill = Type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "AUC medio de Train vs Test por Fold",
       x = "Outer Fold",
       y = "AUC Medio",
       fill = "Tipo de AUC") +
  theme_minimal()

ggplot(inner_fold_metrics, aes(x = factor(Fold), y = AUC)) +
  geom_boxplot() +
  labs(title = "Distribución de AUC por Fold",
       x = "Fold",
       y = "AUC") +
  theme_minimal()

```

```{r, message=FALSE, warning=FALSE}
library(caret)
library(glmnet)
library(rpart)
library(pROC)

double_cross_validation_rpart_Lasso <- function(data, target_name, outer, inner, cps, minsplits, variables) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_cp = numeric(), Best_minsplit = numeric(), 
                                    Best_Variables = character(), stringsAsFactors = FALSE)
  
  inner_fold_metrics <- data.frame(Fold = integer(), Inner_Fold = integer(), 
                                   TP = integer(), TN = integer(), FP = integer(), 
                                   FN = integer(), AUC = numeric(), stringsAsFactors = FALSE)
  
  # Convertir las variables del dataset con model.matrix
  target <- as.numeric(data[[target_name]]) - 1
  features <- data[, setdiff(names(data), target_name)]
  data_matrix <- as.data.frame(model.matrix(~ ., data=features)[,-1])  # Eliminar el intercepto
  data_matrix[[target_name]] <- target
  
  outer_folds <- createFolds(data_matrix[[target_name]], k = outer)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data_matrix[-outer_folds[[outer_index]], ]
    outer_test_data <- data_matrix[outer_folds[[outer_index]], ]
    
    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == 0), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == 1), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == 0), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == 1), "\n")
    
    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner)
    best_model <- NULL
    best_auc <- 0
    best_cp <- NULL
    best_minsplit <- NULL
    best_vars <- colnames(outer_train_data)[!colnames(outer_train_data) %in% target_name]
    
    grid <- expand.grid(cp = cps, minsplit = minsplits)
    
    select_vars_lasso <- function(data, target_name) {
      target <- as.numeric(data[[target_name]])
      features <- data[, setdiff(names(data), target_name)]
      X <- model.matrix(~ ., data=features)[,-1]  # Eliminamos el intercepto
      y <- target
      
      # Ajustamos el modelo Lasso
      lasso_model <- cv.glmnet(X, y, alpha=1)
      lasso_coef <- coef(lasso_model, s = "lambda.min")
      
      # Seleccionamos las variables no nulas
      selected_vars <- rownames(lasso_coef)[lasso_coef[, 1] != 0]
      selected_vars <- setdiff(selected_vars, "(Intercept)")
      cat("Selected variables by Lasso: ", paste(selected_vars, collapse=", "), "\n")
      return(selected_vars)
    }
    
    stepAUC <- function(vars, data, target_name, grid, inner_folds) {
      best_inner_auc <- 0
      best_inner_model <- NULL
      best_inner_cp <- NULL
      best_inner_minsplit <- NULL
      best_inner_vars <- vars
      improved <- TRUE
      
      while (improved && length(vars) > 1) {
        improved <- FALSE
        best_auc_in_step <- best_inner_auc
        
        for (params in 1:nrow(grid)) {
          auc_vals <- numeric()
          
          for (inner_index in seq_along(inner_folds)) {
            inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
            inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
            
            selected_vars <- select_vars_lasso(inner_train_data, target_name)
            formula <- as.formula(paste(target_name, "~", paste(selected_vars, collapse = "+")))
            cat("Testing with formula: ", deparse(formula), "\n")
            cat("Grid parameters - cp: ", grid$cp[params], ", minsplit: ", grid$minsplit[params], "\n")
            
            result <- tryCatch({
              model <- rpart(formula, data = inner_train_data, method = "class", 
                             control = rpart.control(cp = grid$cp[params], minsplit = grid$minsplit[params]))
              
              predictions <- predict(model, inner_test_data, type = "prob")[,2]
              roc_curve <- roc(inner_test_data[[target_name]], predictions)
              auc_vals[inner_index] <- roc_curve$auc
              cat("AUC for inner fold ", inner_index, ": ", roc_curve$auc, "\n")
              
            }, error = function(e) {
              cat("Error with variables: ", paste(selected_vars, collapse = ", "), "\n")
              cat("Error message: ", e$message, "\n")
              auc_vals[inner_index] <- NA  # Set to NA or some other indicator of failure
            })
          }
          
          mean_auc <- mean(auc_vals, na.rm = TRUE)
          cat("Mean AUC for params - cp: ", grid$cp[params], ", minsplit: ", grid$minsplit[params], ": ", mean_auc, "\n")
          if (!is.na(mean_auc) && mean_auc > best_auc_in_step) {
            best_auc_in_step <- mean_auc
            best_inner_model <- model
            best_inner_cp <- grid$cp[params]
            best_inner_minsplit <- grid$minsplit[params]
            best_inner_vars <- selected_vars
            improved <- TRUE
          }
        }
        
        if (improved) {
          vars <- best_inner_vars
          best_inner_auc <- best_auc_in_step
          cat("Vars improved: ", paste(vars, collapse=", "), "\n")
          cat("AUC: ", best_inner_auc, "\n")
        }
      }
      
      list(model = best_inner_model, auc = best_inner_auc, 
           cp = best_inner_cp, minsplit = best_inner_minsplit, 
           vars = best_inner_vars)
    }
    
    best_result <- stepAUC(best_vars, outer_train_data, target_name, grid, inner_folds)
    best_model <- best_result$model
    best_auc <- best_result$auc
    best_cp <- best_result$cp
    best_minsplit <- best_result$minsplit
    best_vars <- best_result$vars
    
    if (is.null(best_model)) {
      cat("Error: No valid model found for outer fold ", outer_index, "\n")
      next
    }

    # Evaluar el mejor modelo en el validation split del inner loop
    inner_auc_vals <- numeric()
    for (inner_index in seq_along(inner_folds)) {
      inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
      inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
      
      predictions <- predict(best_model, inner_test_data, type = "prob")[,2]
      roc_curve <- roc(inner_test_data[[target_name]], predictions)
      inner_auc_vals[inner_index] <- roc_curve$auc
      cat("Inner fold ", inner_index, " AUC: ", roc_curve$auc, "\n")

      pred_class <- ifelse(predictions > 0.35, 1, 0)
      confusion <- table(Actual = inner_test_data[[target_name]], Predicted = pred_class)
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
      auc <- roc_curve$auc
      
      inner_metrics <- setNames(c(outer_index, inner_index, TP, TN, FP, FN, auc), names(inner_fold_metrics))
      inner_fold_metrics <- rbind(inner_fold_metrics, as.data.frame(t(inner_metrics)))
    }
    
    best_inner_auc <- mean(inner_auc_vals, na.rm = TRUE)
    cat("Best inner AUC: ", best_inner_auc, "\n")
    
    predictions <- predict(best_model, outer_test_data, type = "prob")[,2]
    pred_class <- ifelse(predictions > 0.35, 1, 0)  # Asignar clases basadas en umbral
    confusion <- table(Actual = outer_test_data[[target_name]], Predicted = pred_class)
    
    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc <- roc(outer_test_data[[target_name]], predictions)$auc
    
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, 
                          best_cp, best_minsplit, paste(best_vars, collapse = ",")), 
                        names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
    
    cat("Performance:\n", paste(performance_metrics), "\n")
    cat(sprintf("Confusion Matrix for Fold %d:\n", outer_index))
    print(confusion)
    cat(sprintf("Metrics for Fold %d:\n", outer_index))
    cat(sprintf("TP: %d, TN: %d, FP: %d, FN: %d, AUC: %f, Best cp: %f, Best minsplit: %d, Best Variables: %s\n",
                TP, TN, FP, FN, auc, best_cp, best_minsplit, paste(best_vars, collapse = ",")))
  }
  
  list(performance_metrics = performance_metrics, inner_fold_metrics = inner_fold_metrics)
}

# Uso de la función con los parámetros correspondientes
performance_rf <- double_cross_validation_rpart_Lasso(data = data_factor, 
                                                      outer = 5,
                                                      inner = 2,
                                                      target_name = "PCR",
                                                      cps = c(0.01, 0.05, 0.1),
                                                      minsplits = c(5, 10, 15),
                                                      variables = colnames(data_factor)[!colnames(data_factor) %in% "PCR"])
performance_rf
```
```{r}
performance_metrics <- performance_rf$performance_metrics
inner_fold_metrics <- performance_rf$inner_fold_metrics

# Convertir las columnas a numéricas si no lo están
inner_fold_metrics <- inner_fold_metrics %>% 
  mutate(across(c(Fold, Inner_Fold, TP, TN, FP, FN, AUC), as.numeric))

performance_metrics <- performance_metrics %>% 
  mutate(across(c(Fold, TP, TN, FP, FN, AUC, Best_cp, Best_minsplit), as.numeric))

# Calcular el AUC medio de train por cada outer fold
train_auc_means <- inner_fold_metrics %>%
  group_by(Fold) %>%
  summarise(Mean_Train_AUC = mean(AUC, na.rm = TRUE))

# Calcular el AUC medio de test por cada outer fold
test_auc_means <- performance_metrics %>%
  group_by(Fold) %>%
  summarise(Mean_Test_AUC = mean(AUC, na.rm = TRUE))

# Unir los resultados en un solo dataframe
auc_means <- train_auc_means %>%
  left_join(test_auc_means, by = "Fold") %>%
  gather(key = "Type", value = "Mean_AUC", Mean_Train_AUC, Mean_Test_AUC)

# Crear el barplot
ggplot(auc_means, aes(x = factor(Fold), y = Mean_AUC, fill = Type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "AUC medio de Train vs Test por Fold",
       x = "Outer Fold",
       y = "AUC Medio",
       fill = "Tipo de AUC") +
  theme_minimal()

ggplot(inner_fold_metrics, aes(x = factor(Fold), y = AUC)) +
  geom_boxplot() +
  labs(title = "Distribución de AUC por Fold",
       x = "Fold",
       y = "AUC") +
  theme_minimal()

```


## KNN

```{r}
library(caret)
library(class)
library(pROC)

double_cross_validation_knn <- function(data, target_name, outer, inner, ks, ls, variables, threshold) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_k = integer(), Best_l = numeric(), stringsAsFactors = FALSE)
  
  data[[target_name]] <- as.factor(data[[target_name]])
  outer_folds <- createFolds(data[[target_name]], k = outer, list = TRUE, returnTrain = FALSE)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data[-outer_folds[[outer_index]], ]
    outer_test_data <- data[outer_folds[[outer_index]], ]
    
    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == "0"), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == "1"), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == "0"), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == "1"), "\n")
    
    if (sum(outer_train_data[[target_name]] == "0") == 0 || sum(outer_train_data[[target_name]] == "1") == 0) {
      cat("Skipping outer fold due to lack of class diversity\n")
      next
    }
    
    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner, list = TRUE, returnTrain = FALSE)
    best_model <- NULL
    best_auc <- 0
    best_k <- NULL
    best_l <- NULL
    
    grid <- expand.grid(k = ks, l = ls)
    formula <- reformulate(variables, target_name)

    for (params in 1:nrow(grid)) {
      inner_auc <- numeric()

      for (inner_index in seq_along(inner_folds)) {
        inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
        inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]

        if (sum(inner_train_data[[target_name]] == "0") == 0 || sum(inner_train_data[[target_name]] == "1") == 0) {
          cat("Skipping inner fold due to lack of class diversity\n")
          next
        }

        train_x <- inner_train_data[, variables, drop = FALSE]
        test_x <- inner_test_data[, variables, drop = FALSE]
        train_y <- inner_train_data[[target_name]]
        test_y <- inner_test_data[[target_name]]
        
        result <- tryCatch({
          predictions <- knn(train = train_x, test = test_x, cl = train_y, k = grid$k[params], prob = TRUE)
          probabilities <- attr(predictions, "prob")
          probabilities <- ifelse(predictions == "1", probabilities, 1 - probabilities)
          roc_curve <- roc(as.numeric(as.character(test_y)), probabilities)
          inner_auc[inner_index] <- roc_curve$auc
        }, error = function(e) {
          cat("Error with parameters: k =", grid$k[params], ", l =", grid$l[params], "\n")
          cat("Error message: ", e$message, "\n")
          inner_auc[inner_index] <- 0
        })
      }

      if (mean(inner_auc, na.rm = TRUE) > best_auc) {
        best_auc <- mean(inner_auc, na.rm = TRUE)
        best_model <- list(train_x = train_x, train_y = train_y, k = grid$k[params], l = grid$l[params])
        best_k <- grid$k[params]
        best_l <- grid$l[params]
      }
    }

    if (is.null(best_model)) {
      cat("No valid model found for outer fold ", outer_index, "\n")
      next
    }

    test_x <- outer_test_data[, variables, drop = FALSE]
    test_y <- outer_test_data[[target_name]]
    predictions <- knn(train = best_model$train_x, test = test_x, cl = best_model$train_y, k = best_k, prob = TRUE)
    probabilities <- attr(predictions, "prob")
    probabilities <- ifelse(predictions == "1", probabilities, 1 - probabilities)
    pred_class <- ifelse(probabilities > threshold, "1", "0")
    confusion <- table(Actual = test_y, Predicted = pred_class)
    
    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc <- roc(test_y, probabilities)$auc
    
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, best_k, best_l), names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
    
    cat("Performance:", paste(performance_metrics))
    cat(sprintf("Confusion Matrix for Fold %d:\n", outer_index))
    print(confusion)
    cat(sprintf("Metrics for Fold %d:\n", outer_index))
    cat(sprintf("TP: %d, TN: %d, FP: %d, FN: %d, AUC: %f, Best k: %d, Best l: %f\n",
                TP, TN, FP, FN, auc, best_k, best_l))
  }
  
  return(performance_metrics)
}

# Uso de la función con los parámetros correspondientes
performance_knn <- double_cross_validation_knn(data = data_numeric, 
                                               outer = 5,
                                               inner = 2,
                                               target_name = "PCR",
                                               ks = c(10, 16, 20, 25, 30, 32),
                                               ls = c(0, 1, 2),
                                               variables = colnames(data_numeric)[!colnames(data_numeric) %in% "PCR"],
                                               threshold = 0.5)

```


```{r, message=FALSE, warning=FALSE}

double_cross_validation_knn_stepAUC <- function(data, target_name, outer, inner, ks, ls, variables) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_k = integer(), Best_l = numeric(), Best_Variables = character(), stringsAsFactors = FALSE)
  
  inner_fold_metrics <- data.frame(Fold = integer(), Inner_Fold = integer(), 
                                   TP = integer(), TN = integer(), FP = integer(), 
                                   FN = integer(), AUC = numeric(), stringsAsFactors = FALSE)
  
  data[[target_name]] <- as.factor(data[[target_name]])
  outer_folds <- createFolds(data[[target_name]], k = outer, list = TRUE, returnTrain = FALSE)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data[-outer_folds[[outer_index]], ]
    outer_test_data <- data[outer_folds[[outer_index]], ]
    
    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == "0"), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == "1"), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == "0"), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == "1"), "\n")
    
    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner, list = TRUE, returnTrain = FALSE)
    best_model <- NULL
    best_auc <- 0
    best_k <- NULL
    best_l <- NULL
    best_vars <- variables
    
    grid <- expand.grid(k = ks, l = ls)
    
    stepAUC <- function(vars, outer_train_data, target_name, grid, inner_folds) {
      current_vars <- vars
      best_inner_auc <- 0
      best_inner_model <- NULL
      best_inner_k <- NULL
      best_inner_l <- NULL
      best_inner_vars <- current_vars
      improved <- TRUE
      
      while (improved && length(current_vars) > 1) {
        improved <- FALSE
        best_auc_in_step <- best_inner_auc
        
        for (var in current_vars) {
          temp_vars <- setdiff(current_vars, var)
          inner_auc <- numeric()
          
          for (params in 1:nrow(grid)) {
            auc_vals <- numeric()
            
            for (inner_index in seq_along(inner_folds)) {
              inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
              inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
              
              train_x <- inner_train_data[, temp_vars, drop = FALSE]
              test_x <- inner_test_data[, temp_vars, drop = FALSE]
              train_y <- inner_train_data[[target_name]]
              test_y <- inner_test_data[[target_name]]
              
              result <- tryCatch({
                predictions <- knn(train = train_x, test = test_x, cl = train_y, k = grid$k[params], l = grid$l[params], prob = TRUE)
                probabilities <- attr(predictions, "prob")
                roc_curve <- roc(as.numeric(as.character(test_y)), probabilities)
                auc_vals[inner_index] <- roc_curve$auc
                
              }, error = function(e) {
                cat("Error with variables:", paste(temp_vars, collapse = ", "), "\n")
                cat("Error message: ", e$message, "\n")
                auc_vals[inner_index] <- 0  # Set to NA or some other indicator of failure
              })
            }
            
            if (mean(auc_vals, na.rm = TRUE) > best_auc_in_step) {
              best_auc_in_step <- mean(auc_vals, na.rm = TRUE)
              best_inner_model <- list(train_x = train_x, train_y = train_y, k = grid$k[params], l = grid$l[params])
              best_inner_k <- grid$k[params]
              best_inner_l <- grid$l[params]
              best_inner_vars <- temp_vars
              improved <- TRUE
            }
          }
        }
        
        if (improved) {
          current_vars <- best_inner_vars
          best_inner_auc <- best_auc_in_step
          cat("Vars improved: ", paste(current_vars, collapse=", "), "\n")
          cat("AUC: ", paste(best_inner_auc), "\n")
        }
      }
      
      list(model = best_inner_model, auc = best_inner_auc, 
           k = best_inner_k, l = best_inner_l, 
           vars = best_inner_vars)
      
    }
    
    best_result <- stepAUC(variables, outer_train_data, target_name, grid, inner_folds)
    best_model <- best_result$model
    best_auc <- best_result$auc
    best_k <- best_result$k
    best_l <- best_result$l
    best_vars <- best_result$vars

    # Evaluar el mejor modelo en el validation split del inner loop
    inner_auc_vals <- numeric()
    for (inner_index in seq_along(inner_folds)) {
      inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
      inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
      
      train_x <- best_model$train_x
      test_x <- inner_test_data[, best_vars, drop = FALSE]
      train_y <- best_model$train_y
      test_y <- inner_test_data[[target_name]]
      
      predictions <- knn(train = train_x, test = test_x, cl = train_y, k = best_k, l = best_l, prob = TRUE)
      probabilities <- attr(predictions, "prob")
      roc_curve <- roc(as.numeric(as.character(test_y)), probabilities)
      inner_auc_vals[inner_index] <- roc_curve$auc

      pred_class <- ifelse(probabilities > 0.35, "1", "0")
      confusion <- table(Actual = test_y, Predicted = pred_class)
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
      auc <- roc_curve$auc
      
      inner_metrics <- setNames(c(outer_index, inner_index, TP, TN, FP, FN, auc), names(inner_fold_metrics))
      inner_fold_metrics <- rbind(inner_fold_metrics, as.data.frame(t(inner_metrics)))
    }
    
    best_inner_auc <- mean(inner_auc_vals, na.rm = TRUE)
    
    test_x <- outer_test_data[, best_vars, drop = FALSE]
    test_y <- outer_test_data[[target_name]]
    predictions <- knn(train = best_model$train_x, test = test_x, cl = best_model$train_y, k = best_k, l = best_l, prob = TRUE)
    probabilities <- attr(predictions, "prob")
    pred_class <- ifelse(probabilities > 0.35, "1", "0")
    confusion <- table(Actual = test_y, Predicted = pred_class)
    
    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc <- roc(test_y, probabilities)$auc
    
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, 
                          best_k, best_l, paste(best_vars, collapse = ",")), 
                        names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
    
    cat("Performance:", paste(performance_metrics))
    cat(sprintf("Confusion Matrix for Fold %d:\n", outer_index))
    print(confusion)
    cat(sprintf("Metrics for Fold %d:\n", outer_index))
    cat(sprintf("TP: %d, TN: %d, FP: %d, FN: %d, AUC: %f, Best k: %d, Best l: %f, Best Variables: %s\n",
                TP, TN, FP, FN, auc, best_k, best_l, paste(best_vars, collapse = ",")))
  }
  
  list(performance_metrics = performance_metrics, inner_fold_metrics = inner_fold_metrics)
}

# Uso de la función con los parámetros correspondientes
performance_knn <- double_cross_validation_knn_stepAUC(data = data_numeric, 
                                                       outer = 5,
                                                       inner = 2,
                                                       target_name = "PCR",
                                                       ks = c(10, 16, 20, 25, 30, 32),
                                                       ls = c(0, 1, 2),
                                                       variables = colnames(data_numeric)[!colnames(data_numeric) %in% "PCR"])

performance_knn

```

```{r}

performance_metrics <- performance_knn$performance_metrics
inner_fold_metrics <- performance_knn$inner_fold_metrics

# Convertir las columnas a numéricas si no lo están
inner_fold_metrics <- inner_fold_metrics %>% 
  mutate(across(c(Fold, Inner_Fold, TP, TN, FP, FN, AUC), as.numeric))

performance_metrics <- performance_metrics %>% 
  mutate(across(c(Fold, TP, TN, FP, FN, AUC, Best_k, Best_l), as.numeric))

# Calcular el AUC medio de train por cada outer fold
train_auc_means <- inner_fold_metrics %>%
  group_by(Fold) %>%
  summarise(Mean_Train_AUC = mean(AUC, na.rm = TRUE))

# Calcular el AUC medio de test por cada outer fold
test_auc_means <- performance_metrics %>%
  group_by(Fold) %>%
  summarise(Mean_Test_AUC = mean(AUC, na.rm = TRUE))

# Unir los resultados en un solo dataframe
auc_means <- train_auc_means %>%
  left_join(test_auc_means, by = "Fold") %>%
  gather(key = "Type", value = "Mean_AUC", Mean_Train_AUC, Mean_Test_AUC)

# Crear el barplot
ggplot(auc_means, aes(x = factor(Fold), y = Mean_AUC, fill = Type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "AUC medio de Train vs Test por Fold",
       x = "Outer Fold",
       y = "AUC Medio",
       fill = "Tipo de AUC") +
  theme_minimal()

ggplot(inner_fold_metrics, aes(x = factor(Fold), y = AUC)) +
  geom_boxplot() +
  labs(title = "Distribución de AUC por Fold",
       x = "Fold",
       y = "AUC") +
  theme_minimal()

```
```{r, message=FALSE, warning=FALSE}
library(caret)
library(glmnet)
library(class)
library(pROC)

double_cross_validation_knn_lasso <- function(data, target_name, outer, inner, ks, ls, variables) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_k = integer(), Best_l = integer(), 
                                    Best_Variables = character(), stringsAsFactors = FALSE)
  
  inner_fold_metrics <- data.frame(Fold = integer(), Inner_Fold = integer(), 
                                   TP = integer(), TN = integer(), FP = integer(), 
                                   FN = integer(), AUC = numeric(), stringsAsFactors = FALSE)
  
  # Convertir las variables del dataset con model.matrix
  target <- as.numeric(data[[target_name]]) - 1
  features <- data[, setdiff(names(data), target_name)]
  data_matrix <- as.data.frame(model.matrix(~ ., data=features)[,-1])  # Eliminar el intercepto
  data_matrix[[target_name]] <- target
  
  outer_folds <- createFolds(data_matrix[[target_name]], k = outer)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data_matrix[-outer_folds[[outer_index]], ]
    outer_test_data <- data_matrix[outer_folds[[outer_index]], ]
    
    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == 0), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == 1), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == 0), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == 1), "\n")
    
    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner)
    best_model <- NULL
    best_auc <- 0
    best_k <- NULL
    best_l <- NULL
    best_vars <- colnames(outer_train_data)[!colnames(outer_train_data) %in% target_name]
    
    grid <- expand.grid(k = ks, l = ls)
    
    select_vars_lasso <- function(data, target_name) {
      target <- as.numeric(data[[target_name]])
      features <- data[, setdiff(names(data), target_name)]
      X <- model.matrix(~ ., data=features)[,-1]  # Eliminamos el intercepto
      y <- target
      
      # Ajustamos el modelo Lasso
      lasso_model <- cv.glmnet(X, y, alpha=1)
      lasso_coef <- coef(lasso_model, s = "lambda.min")
      
      # Seleccionamos las variables no nulas
      selected_vars <- rownames(lasso_coef)[lasso_coef[, 1] != 0]
      selected_vars <- setdiff(selected_vars, "(Intercept)")
      cat("Selected variables by Lasso: ", paste(selected_vars, collapse=", "), "\n")
      return(selected_vars)
    }
    
    stepAUC <- function(vars, data, target_name, grid, inner_folds) {
      best_inner_auc <- 0
      best_inner_model <- NULL
      best_inner_k <- NULL
      best_inner_l <- NULL
      best_inner_vars <- vars
      improved <- TRUE
      
      while (improved && length(vars) > 1) {
        improved <- FALSE
        best_auc_in_step <- best_inner_auc
        
        for (params in 1:nrow(grid)) {
          auc_vals <- numeric()
          
          for (inner_index in seq_along(inner_folds)) {
            inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
            inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
            
            selected_vars <- select_vars_lasso(inner_train_data, target_name)
            formula <- as.formula(paste(target_name, "~", paste(selected_vars, collapse = "+")))
            cat("Testing with formula: ", deparse(formula), "\n")
            cat("Grid parameters - k: ", grid$k[params], ", l: ", grid$l[params], "\n")
            
            result <- tryCatch({
              train_x <- inner_train_data[, selected_vars, drop = FALSE]
              test_x <- inner_test_data[, selected_vars, drop = FALSE]
              train_y <- inner_train_data[[target_name]]
              test_y <- inner_test_data[[target_name]]
              
              predictions <- knn(train_x, test_x, train_y, k = grid$k[params], l = grid$l[params], prob = TRUE)
              prob_predictions <- attr(predictions, "prob")
              prob_predictions <- ifelse(predictions == "1", prob_predictions, 1 - prob_predictions)
              roc_curve <- roc(inner_test_data[[target_name]], prob_predictions)
              auc_vals[inner_index] <- roc_curve$auc
              cat("AUC for inner fold ", inner_index, ": ", roc_curve$auc, "\n")
              
            }, error = function(e) {
              cat("Error with variables: ", paste(selected_vars, collapse = ", "), "\n")
              cat("Error message: ", e$message, "\n")
              auc_vals[inner_index] <- NA  # Set to NA or some other indicator of failure
            })
          }
          
          mean_auc <- mean(auc_vals, na.rm = TRUE)
          cat("Mean AUC for params - k: ", grid$k[params], ", l: ", grid$l[params], ": ", mean_auc, "\n")
          if (!is.na(mean_auc) && mean_auc > best_auc_in_step) {
            best_auc_in_step <- mean_auc
            best_inner_model <- list(k = grid$k[params], l = grid$l[params])
            best_inner_k <- grid$k[params]
            best_inner_l <- grid$l[params]
            best_inner_vars <- selected_vars
            improved <- TRUE
          }
        }
        
        if (improved) {
          vars <- best_inner_vars
          best_inner_auc <- best_auc_in_step
          cat("Vars improved: ", paste(vars, collapse=", "), "\n")
          cat("AUC: ", best_inner_auc, "\n")
        }
      }
      
      list(model = best_inner_model, auc = best_inner_auc, 
           k = best_inner_k, l = best_inner_l, 
           vars = best_inner_vars)
    }
    
    best_result <- stepAUC(best_vars, outer_train_data, target_name, grid, inner_folds)
    best_model <- best_result$model
    best_auc <- best_result$auc
    best_k <- best_result$k
    best_l <- best_result$l
    best_vars <- best_result$vars
    
    if (is.null(best_model)) {
      cat("Error: No valid model found for outer fold ", outer_index, "\n")
      next
    }

    # Evaluar el mejor modelo en el validation split del inner loop
    inner_auc_vals <- numeric()
    for (inner_index in seq_along(inner_folds)) {
      inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
      inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
      
      train_x <- inner_train_data[, best_vars, drop = FALSE]
      test_x <- inner_test_data[, best_vars, drop = FALSE]
      train_y <- inner_train_data[[target_name]]
      test_y <- inner_test_data[[target_name]]
      
      predictions <- knn(train_x, test_x, train_y, k = best_k, l = best_l, prob = TRUE)
      prob_predictions <- attr(predictions, "prob")
      prob_predictions <- ifelse(predictions == "1", prob_predictions, 1 - prob_predictions)
      roc_curve <- roc(inner_test_data[[target_name]], prob_predictions)
      inner_auc_vals[inner_index] <- roc_curve$auc
      cat("Inner fold ", inner_index, " AUC: ", roc_curve$auc, "\n")

      pred_class <- ifelse(prob_predictions > 0.35, 1, 0)
      confusion <- table(Actual = inner_test_data[[target_name]], Predicted = pred_class)
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
      auc <- roc_curve$auc
      
      inner_metrics <- setNames(c(outer_index, inner_index, TP, TN, FP, FN, auc), names(inner_fold_metrics))
      inner_fold_metrics <- rbind(inner_fold_metrics, as.data.frame(t(inner_metrics)))
    }
    
    best_inner_auc <- mean(inner_auc_vals, na.rm = TRUE)
    cat("Best inner AUC: ", best_inner_auc, "\n")
    
    train_x <- outer_train_data[, best_vars, drop = FALSE]
    test_x <- outer_test_data[, best_vars, drop = FALSE]
    train_y <- outer_train_data[[target_name]]
    test_y <- outer_test_data[[target_name]]
    
    predictions <- knn(train_x, test_x, train_y, k = best_k, l = best_l, prob = TRUE)
    prob_predictions <- attr(predictions, "prob")
    prob_predictions <- ifelse(predictions == "1", prob_predictions, 1 - prob_predictions)
    pred_class <- ifelse(prob_predictions > 0.35, 1, 0)  # Asignar clases basadas en umbral
    confusion <- table(Actual = outer_test_data[[target_name]], Predicted = pred_class)
    
    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc <- roc(outer_test_data[[target_name]], prob_predictions)$auc
    
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, 
                          best_k, best_l, paste(best_vars, collapse = ",")), 
                        names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
    
    cat("Performance:\n", paste(performance_metrics), "\n")
    cat(sprintf("Confusion Matrix for Fold %d:\n", outer_index))
    print(confusion)
    cat(sprintf("Metrics for Fold %d:\n", outer_index))
    cat(sprintf("TP: %d, TN: %d, FP: %d, FN: %d, AUC: %f, Best k: %d, Best l: %d, Best Variables: %s\n",
                TP, TN, FP, FN, auc, best_k, best_l, paste(best_vars, collapse = ",")))
  }
  
  list(performance_metrics = performance_metrics, inner_fold_metrics = inner_fold_metrics)
}

# Uso de la función con los parámetros correspondientes
performance_knn <- double_cross_validation_knn_lasso(data = data_factor,
                                                     outer = 5,
                                                     inner = 2,
                                                     target_name = "PCR",
                                                     ks = c(10, 16, 20, 25, 30, 32),
                                                     ls = c(0, 1, 2),
                                                     variables = colnames(data_factor)[!colnames(data_factor) %in% "PCR"])
performance_knn
```
```{r}
performance_metrics <- performance_knn$performance_metrics
inner_fold_metrics <- performance_knn$inner_fold_metrics

# Convertir las columnas a numéricas si no lo están
inner_fold_metrics <- inner_fold_metrics %>% 
  mutate(across(c(Fold, Inner_Fold, TP, TN, FP, FN, AUC), as.numeric))

performance_metrics <- performance_metrics %>% 
  mutate(across(c(Fold, TP, TN, FP, FN, AUC, Best_k, Best_l), as.numeric))

# Calcular el AUC medio de train por cada outer fold
train_auc_means <- inner_fold_metrics %>%
  group_by(Fold) %>%
  summarise(Mean_Train_AUC = mean(AUC, na.rm = TRUE))

# Calcular el AUC medio de test por cada outer fold
test_auc_means <- performance_metrics %>%
  group_by(Fold) %>%
  summarise(Mean_Test_AUC = mean(AUC, na.rm = TRUE))

# Unir los resultados en un solo dataframe
auc_means <- train_auc_means %>%
  left_join(test_auc_means, by = "Fold") %>%
  gather(key = "Type", value = "Mean_AUC", Mean_Train_AUC, Mean_Test_AUC)

# Crear el barplot
ggplot(auc_means, aes(x = factor(Fold), y = Mean_AUC, fill = Type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "AUC medio de Train vs Test por Fold",
       x = "Outer Fold",
       y = "AUC Medio",
       fill = "Tipo de AUC") +
  theme_minimal()

ggplot(inner_fold_metrics, aes(x = factor(Fold), y = AUC)) +
  geom_boxplot() +
  labs(title = "Distribución de AUC por Fold",
       x = "Fold",
       y = "AUC") +
  theme_minimal()
```



## adaboost

```{r, message=FALSE, warning=FALSE}
library(caret)
library(pROC)
library(mboost)

double_cross_validation_adaboost_stepAUC <- function(data, target_name, outer, inner, iterations, variables) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_Iterations = integer(), Best_Variables = character(), stringsAsFactors = FALSE)
  
  inner_fold_metrics <- data.frame(Fold = integer(), Inner_Fold = integer(), 
                                   TP = integer(), TN = integer(), FP = integer(), 
                                   FN = integer(), AUC = numeric(), stringsAsFactors = FALSE)
  
  data[[target_name]] <- as.factor(data[[target_name]])
  outer_folds <- createFolds(data[[target_name]], k = outer, list = TRUE, returnTrain = FALSE)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data[-outer_folds[[outer_index]], ]
    outer_test_data <- data[outer_folds[[outer_index]], ]
    
    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == "0"), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == "1"), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == "0"), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == "1"), "\n")
    
    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner, list = TRUE, returnTrain = FALSE)
    best_model <- NULL
    best_auc <- 0
    best_iterations <- NULL
    best_vars <- variables
    
    stepAUC <- function(vars, outer_train_data, target_name, iterations, inner_folds) {
      current_vars <- vars
      best_inner_auc <- 0
      best_inner_model <- NULL
      best_inner_iterations <- NULL
      best_inner_vars <- current_vars
      improved <- TRUE
      
      while (improved && length(current_vars) > 1) {
        improved <- FALSE
        best_auc_in_step <- best_inner_auc
        
        for (var in current_vars) {
          temp_vars <- setdiff(current_vars, var)
          inner_auc <- numeric()
          
          for (iter in iterations) {
            auc_vals <- numeric()
            
            for (inner_index in seq_along(inner_folds)) {
              inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
              inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
              
              result <- tryCatch({
                model <- glmboost(as.formula(paste(target_name, "~", paste(temp_vars, collapse = "+"))),
                                  data = inner_train_data, family = Binomial(),
                                  control = boost_control(mstop = iter))
                
                predictions <- predict(model, newdata = inner_test_data, type = "response")
                roc_curve <- roc(inner_test_data[[target_name]], predictions)
                auc_vals[inner_index] <- roc_curve$auc
                
              }, error = function(e) {
                cat("Error with variables:", paste(temp_vars, collapse = ", "), "\n")
                auc_vals[inner_index] <- 0
              })
            }
            
            if (mean(auc_vals, na.rm = TRUE) > best_auc_in_step) {
              best_auc_in_step <- mean(auc_vals, na.rm = TRUE)
              best_inner_model <- model
              best_inner_iterations <- iter
              best_inner_vars <- temp_vars
              improved <- TRUE
            }
          }
        }
        
        if (improved) {
          current_vars <- best_inner_vars
          best_inner_auc <- best_auc_in_step
          cat("Vars improved: ", paste(current_vars, collapse=", "), "\n")
          cat("AUC: ", paste(best_inner_auc), "\n")
        }
      }
      
      list(model = best_inner_model, auc = best_inner_auc, 
           iterations = best_inner_iterations, 
           vars = best_inner_vars)
    }
    
    best_result <- stepAUC(variables, outer_train_data, target_name, iterations, inner_folds)
    best_model <- best_result$model
    best_auc <- best_result$auc
    best_iterations <- best_result$iterations
    best_vars <- best_result$vars

    # Evaluar el mejor modelo en el validation split del inner loop
    inner_auc_vals <- numeric()
    for (inner_index in seq_along(inner_folds)) {
      inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
      inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
      
      predictions <- predict(best_model, newdata = inner_test_data, type = "response")
      roc_curve <- roc(inner_test_data[[target_name]], predictions)
      inner_auc_vals[inner_index] <- roc_curve$auc

      pred_class <- ifelse(predictions > 0.35, "1", "0")
      confusion <- table(Actual = inner_test_data[[target_name]], Predicted = pred_class)
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
      auc <- roc_curve$auc
      
      inner_metrics <- setNames(c(outer_index, inner_index, TP, TN, FP, FN, auc), names(inner_fold_metrics))
      inner_fold_metrics <- rbind(inner_fold_metrics, as.data.frame(t(inner_metrics)))
    }
    
    best_inner_auc <- mean(inner_auc_vals, na.rm = TRUE)
    
    predictions <- predict(best_model, outer_test_data, type = "response")
    pred_class <- ifelse(predictions > 0.35, "1", "0")
    confusion <- table(Actual = outer_test_data[[target_name]], Predicted = pred_class)
    
    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc <- roc(outer_test_data[[target_name]], predictions)$auc
    
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, 
                          best_iterations, paste(best_vars, collapse = ",")), 
                        names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
    
    cat("Performance:", paste(performance_metrics))
    cat(sprintf("Confusion Matrix for Fold %d:\n", outer_index))
    print(confusion)
    cat(sprintf("Metrics for Fold %d:\n", outer_index))
    cat(sprintf("TP: %d, TN: %d, FP: %d, FN: %d, AUC: %f, Best Iterations: %d, Best Variables: %s\n",
                TP, TN, FP, FN, auc, best_iterations, paste(best_vars, collapse = ",")))
  }
  
  list(performance_metrics = performance_metrics, inner_fold_metrics = inner_fold_metrics)
}

# Uso de la función con los parámetros correspondientes
performance_adaboost <- double_cross_validation_adaboost_stepAUC(data = data_factor, 
                                                                 outer = 5,
                                                                 inner = 2,
                                                                 target_name = "PCR",
                                                                 iterations = c(50, 100, 150),
                                                                 variables = colnames(data)[!colnames(data) %in% "PCR"])
performance_adaboost
```

```{r}

performance_metrics <- performance_adaboost$performance_metrics
inner_fold_metrics <- performance_adaboost$inner_fold_metrics

# Convertir las columnas a numéricas si no lo están
inner_fold_metrics <- inner_fold_metrics %>% 
  mutate(across(c(Fold, Inner_Fold, TP, TN, FP, FN, AUC), as.numeric))

performance_metrics <- performance_metrics %>% 
  mutate(across(c(Fold, TP, TN, FP, FN, AUC, Best_Iterations), as.numeric))

# Calcular el AUC medio de train por cada outer fold
train_auc_means <- inner_fold_metrics %>%
  group_by(Fold) %>%
  summarise(Mean_Train_AUC = mean(AUC, na.rm = TRUE))

# Calcular el AUC medio de test por cada outer fold
test_auc_means <- performance_metrics %>%
  group_by(Fold) %>%
  summarise(Mean_Test_AUC = mean(AUC, na.rm = TRUE))

# Unir los resultados en un solo dataframe
auc_means <- train_auc_means %>%
  left_join(test_auc_means, by = "Fold") %>%
  gather(key = "Type", value = "Mean_AUC", Mean_Train_AUC, Mean_Test_AUC)

# Crear el barplot
ggplot(auc_means, aes(x = factor(Fold), y = Mean_AUC, fill = Type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "AUC medio de Train vs Test por Fold",
       x = "Outer Fold",
       y = "AUC Medio",
       fill = "Tipo de AUC") +
  theme_minimal()

ggplot(inner_fold_metrics, aes(x = factor(Fold), y = AUC)) +
  geom_boxplot() +
  labs(title = "Distribución de AUC por Fold",
       x = "Fold",
       y = "AUC") +
  theme_minimal()

```

```{r, message=FALSE, warning=FALSE}
library(caret)
library(glmnet)
library(mboost)
library(pROC)

double_cross_validation_adaboost_lasso <- function(data, target_name, outer, inner, iterations, variables) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_Iteration = integer(), 
                                    Best_Variables = character(), stringsAsFactors = FALSE)
  
  inner_fold_metrics <- data.frame(Fold = integer(), Inner_Fold = integer(), 
                                   TP = integer(), TN = integer(), FP = integer(), 
                                   FN = integer(), AUC = numeric(), stringsAsFactors = FALSE)
  
  # Convertir las variables del dataset con model.matrix
  target <- as.factor(data[[target_name]])  # Aseguramos que la variable de respuesta sea factor
  features <- data[, setdiff(names(data), target_name)]
  data_matrix <- as.data.frame(model.matrix(~ ., data=features)[,-1])  # Eliminar el intercepto
  data_matrix[[target_name]] <- target
  
  outer_folds <- createFolds(data_matrix[[target_name]], k = outer)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data_matrix[-outer_folds[[outer_index]], ]
    outer_test_data <- data_matrix[outer_folds[[outer_index]], ]
    
    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == 0), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == 1), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == 0), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == 1), "\n")
    
    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner)
    best_model <- NULL
    best_auc <- 0
    best_iteration <- NULL
    best_vars <- colnames(outer_train_data)[!colnames(outer_train_data) %in% target_name]
    
    grid <- expand.grid(iteration = iterations)
    
    select_vars_lasso <- function(data, target_name) {
      target <- as.numeric(data[[target_name]]) - 1  # Convertimos a numérico para Lasso
      features <- data[, setdiff(names(data), target_name)]
      X <- model.matrix(~ ., data=features)[,-1]  # Eliminamos el intercepto
      y <- target
      
      # Ajustamos el modelo Lasso
      lasso_model <- cv.glmnet(X, y, alpha=1)
      lasso_coef <- coef(lasso_model, s = "lambda.min")
      
      # Seleccionamos las variables no nulas
      selected_vars <- rownames(lasso_coef)[lasso_coef[, 1] != 0]
      selected_vars <- setdiff(selected_vars, "(Intercept)")
      cat("Selected variables by Lasso: ", paste(selected_vars, collapse=", "), "\n")
      return(selected_vars)
    }
    
    stepAUC <- function(vars, data, target_name, grid, inner_folds) {
      best_inner_auc <- 0
      best_inner_model <- NULL
      best_inner_iteration <- NULL
      best_inner_vars <- vars
      improved <- TRUE
      
      while (improved && length(vars) > 1) {
        improved <- FALSE
        best_auc_in_step <- best_inner_auc
        
        for (params in 1:nrow(grid)) {
          auc_vals <- numeric()
          
          for (inner_index in seq_along(inner_folds)) {
            inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
            inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
            
            selected_vars <- select_vars_lasso(inner_train_data, target_name)
            formula <- as.formula(paste(target_name, "~", paste(selected_vars, collapse = "+")))
            cat("Testing with formula: ", deparse(formula), "\n")
            cat("Grid parameters - iteration: ", grid$iteration[params], "\n")
            
            result <- tryCatch({
              model <- blackboost(formula, data = inner_train_data, family = Binomial(), 
                                  control = boost_control(mstop = grid$iteration[params]))
              
              predictions <- predict(model, inner_test_data, type = "response")
              roc_curve <- roc(inner_test_data[[target_name]], predictions)
              auc_vals[inner_index] <- roc_curve$auc
              cat("AUC for inner fold ", inner_index, ": ", roc_curve$auc, "\n")
              
            }, error = function(e) {
              cat("Error with variables: ", paste(selected_vars, collapse = ", "), "\n")
              cat("Error message: ", e$message, "\n")
              auc_vals[inner_index] <- NA  # Set to NA or some other indicator of failure
            })
          }
          
          mean_auc <- mean(auc_vals, na.rm = TRUE)
          cat("Mean AUC for params - iteration: ", grid$iteration[params], ": ", mean_auc, "\n")
          if (!is.na(mean_auc) && mean_auc > best_auc_in_step) {
            best_auc_in_step <- mean_auc
            best_inner_model <- model
            best_inner_iteration <- grid$iteration[params]
            best_inner_vars <- selected_vars
            improved <- TRUE
          }
        }
        
        if (improved) {
          vars <- best_inner_vars
          best_inner_auc <- best_auc_in_step
          cat("Vars improved: ", paste(vars, collapse=", "), "\n")
          cat("AUC: ", best_inner_auc, "\n")
        }
      }
      
      list(model = best_inner_model, auc = best_inner_auc, 
           iteration = best_inner_iteration, vars = best_inner_vars)
    }
    
    best_result <- stepAUC(best_vars, outer_train_data, target_name, grid, inner_folds)
    best_model <- best_result$model
    best_auc <- best_result$auc
    best_iteration <- best_result$iteration
    best_vars <- best_result$vars
    
    if (is.null(best_model)) {
      cat("Error: No valid model found for outer fold ", outer_index, "\n")
      next
    }

    # Evaluar el mejor modelo en el validation split del inner loop
    inner_auc_vals <- numeric()
    for (inner_index in seq_along(inner_folds)) {
      inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
      inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
      
      predictions <- predict(best_model, inner_test_data, type = "response")
      roc_curve <- roc(inner_test_data[[target_name]], predictions)
      inner_auc_vals[inner_index] <- roc_curve$auc
      cat("Inner fold ", inner_index, " AUC: ", roc_curve$auc, "\n")

      pred_class <- ifelse(predictions > 0.35, 1, 0)
      confusion <- table(Actual = inner_test_data[[target_name]], Predicted = pred_class)
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
      auc <- roc_curve$auc
      
      inner_metrics <- setNames(c(outer_index, inner_index, TP, TN, FP, FN, auc), names(inner_fold_metrics))
      inner_fold_metrics <- rbind(inner_fold_metrics, as.data.frame(t(inner_metrics)))
    }
    
    best_inner_auc <- mean(inner_auc_vals, na.rm = TRUE)
    cat("Best inner AUC: ", best_inner_auc, "\n")
    
    predictions <- predict(best_model, outer_test_data, type = "response")
    pred_class <- ifelse(predictions > 0.35, 1, 0)  # Asignar clases basadas en umbral
    confusion <- table(Actual = outer_test_data[[target_name]], Predicted = pred_class)
    
    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc <- roc(outer_test_data[[target_name]], predictions)$auc
    
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, 
                          best_iteration, paste(best_vars, collapse = ",")), 
                        names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
    
    cat("Performance:\n", paste(performance_metrics), "\n")
    cat(sprintf("Confusion Matrix for Fold %d:\n", outer_index))
    print(confusion)
    cat(sprintf("Metrics for Fold %d:\n", outer_index))
    cat(sprintf("TP: %d, TN: %d, FP: %d, FN: %d, AUC: %f, Best Iteration: %d, Best Variables: %s\n",
                TP, TN, FP, FN, auc, best_iteration, paste(best_vars, collapse = ",")))
  }
  
  list(performance_metrics = performance_metrics, inner_fold_metrics = inner_fold_metrics)
}

# Uso de la función con los parámetros correspondientes
performance_adaboost <- double_cross_validation_adaboost_lasso(data = data_factor, 
                                                               outer = 5,
                                                               inner = 2,
                                                               target_name = "PCR",
                                                               iterations = c(50, 100, 150),
                                                               variables = colnames(data_factor)[!colnames(data_factor) %in% "PCR"])

performance_adaboost
```

```{r}
performance_metrics <- performance_adaboost$performance_metrics
inner_fold_metrics <- performance_adaboost$inner_fold_metrics

# Convertir las columnas a numéricas si no lo están
inner_fold_metrics <- inner_fold_metrics %>% 
  mutate(across(c(Fold, Inner_Fold, TP, TN, FP, FN, AUC), as.numeric))

performance_metrics <- performance_metrics %>% 
  mutate(across(c(Fold, TP, TN, FP, FN, AUC, Best_Iteration), as.numeric))

# Calcular el AUC medio de train por cada outer fold
train_auc_means <- inner_fold_metrics %>%
  group_by(Fold) %>%
  summarise(Mean_Train_AUC = mean(AUC, na.rm = TRUE))

# Calcular el AUC medio de test por cada outer fold
test_auc_means <- performance_metrics %>%
  group_by(Fold) %>%
  summarise(Mean_Test_AUC = mean(AUC, na.rm = TRUE))

# Unir los resultados en un solo dataframe
auc_means <- train_auc_means %>%
  left_join(test_auc_means, by = "Fold") %>%
  gather(key = "Type", value = "Mean_AUC", Mean_Train_AUC, Mean_Test_AUC)

# Crear el barplot
ggplot(auc_means, aes(x = factor(Fold), y = Mean_AUC, fill = Type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "AUC medio de Train vs Test por Fold",
       x = "Outer Fold",
       y = "AUC Medio",
       fill = "Tipo de AUC") +
  theme_minimal()

ggplot(inner_fold_metrics, aes(x = factor(Fold), y = AUC)) +
  geom_boxplot() +
  labs(title = "Distribución de AUC por Fold",
       x = "Fold",
       y = "AUC") +
  theme_minimal()
```


## naive bayes

```{r, message=FALSE, warning=FALSE}
library(caret)
library(glmnet)
library(e1071)
library(pROC)

double_cross_validation_naiveBayes_stepAUC <- function(data, target_name, outer, inner, variables) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_Variables = character(), stringsAsFactors = FALSE)
  
  inner_fold_metrics <- data.frame(Fold = integer(), Inner_Fold = integer(), 
                                   TP = integer(), TN = integer(), FP = integer(), 
                                   FN = integer(), AUC = numeric(), stringsAsFactors = FALSE)
  
  data[[target_name]] <- as.factor(data[[target_name]])
  outer_folds <- createFolds(data[[target_name]], k = outer, list = TRUE, returnTrain = FALSE)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data[-outer_folds[[outer_index]], ]
    outer_test_data <- data[outer_folds[[outer_index]], ]
    
    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == "0"), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == "1"), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == "0"), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == "1"), "\n")
    
    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner, list = TRUE, returnTrain = FALSE)
    best_model <- NULL
    best_auc <- 0
    best_vars <- variables
    
    stepAUC <- function(vars, outer_train_data, target_name, inner_folds) {
      current_vars <- vars
      best_inner_auc <- 0
      best_inner_model <- NULL
      best_inner_vars <- current_vars
      improved <- TRUE
      
      while (improved && length(current_vars) > 1) {
        improved <- FALSE
        best_auc_in_step <- best_inner_auc
        
        for (var in current_vars) {
          temp_vars <- setdiff(current_vars, var)
          inner_auc <- numeric()
          
          for (inner_index in seq_along(inner_folds)) {
            inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
            inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
            
            result <- tryCatch({
              model <- naiveBayes(as.formula(paste(target_name, "~", paste(temp_vars, collapse = "+"))),
                                  data = inner_train_data)
              
              predictions <- predict(model, newdata = inner_test_data, type = "raw")[, 2]
              roc_curve <- roc(inner_test_data[[target_name]], predictions)
              inner_auc[inner_index] <- roc_curve$auc
              
            }, error = function(e) {
              cat("Error with variables:", paste(temp_vars, collapse = ", "), "\n")
              inner_auc[inner_index] <- NA  # Set to NA or some other indicator of failure
            })
          }
          
          mean_inner_auc <- mean(inner_auc, na.rm = TRUE)
          if (!is.na(mean_inner_auc) && mean_inner_auc > best_auc_in_step) {
            best_auc_in_step <- mean_inner_auc
            best_inner_model <- model
            best_inner_vars <- temp_vars
            improved <- TRUE
          }
        }
        
        if (improved) {
          current_vars <- best_inner_vars
          best_inner_auc <- best_auc_in_step
          cat("Vars improved: ", paste(current_vars, collapse=", "), "\n")
          cat("AUC: ", paste(best_inner_auc), "\n")
        }
      }
      
      list(model = best_inner_model, auc = best_inner_auc, vars = best_inner_vars)
    }
    
    best_result <- stepAUC(variables, outer_train_data, target_name, inner_folds)
    best_model <- best_result$model
    best_auc <- best_result$auc
    best_vars <- best_result$vars

    if (is.null(best_model)) {
      cat("Error: No valid model found for outer fold ", outer_index, "\n")
      next
    }

    # Evaluar el mejor modelo en el validation split del inner loop
    inner_auc_vals <- numeric()
    for (inner_index in seq_along(inner_folds)) {
      inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
      inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
      
      predictions <- predict(best_model, newdata = inner_test_data, type = "raw")[, 2]
      roc_curve <- roc(inner_test_data[[target_name]], predictions)
      inner_auc_vals[inner_index] <- roc_curve$auc

      pred_class <- ifelse(predictions > 0.35, "1", "0")
      confusion <- table(Actual = inner_test_data[[target_name]], Predicted = pred_class)
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
      auc <- roc_curve$auc
      
      inner_metrics <- setNames(c(outer_index, inner_index, TP, TN, FP, FN, auc), names(inner_fold_metrics))
      inner_fold_metrics <- rbind(inner_fold_metrics, as.data.frame(t(inner_metrics)))
    }
    
    best_inner_auc <- mean(inner_auc_vals, na.rm = TRUE)
    
    predictions <- predict(best_model, outer_test_data, type = "raw")[, 2]
    pred_class <- ifelse(predictions > 0.35, "1", "0")
    confusion <- table(Actual = outer_test_data[[target_name]], Predicted = pred_class)
    
    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc <- roc(outer_test_data[[target_name]], predictions)$auc
    
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, 
                          paste(best_vars, collapse = ",")), 
                        names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
    
    cat("Performance:", paste(performance_metrics))
    cat(sprintf("Confusion Matrix for Fold %d:\n", outer_index))
    print(confusion)
    cat(sprintf("Metrics for Fold %d:\n", outer_index))
    cat(sprintf("TP: %d, TN: %d, FP: %d, FN: %d, AUC: %f, Best Variables: %s\n",
                TP, TN, FP, FN, auc, paste(best_vars, collapse = ",")))
  }
  
  list(performance_metrics = performance_metrics, inner_fold_metrics = inner_fold_metrics)
}

# Uso de la función con los parámetros correspondientes
performance_naiveBayes <- double_cross_validation_naiveBayes_stepAUC(data = data_factor, 
                                                                     outer = 5,
                                                                     inner = 2,
                                                                     target_name = "PCR",
                                                                     variables = colnames(data_factor)[!colnames(data_factor) %in% "PCR"])
performance_naiveBayes
```


```{r}
performance_metrics <- performance_naiveBayes$performance_metrics
inner_fold_metrics <- performance_naiveBayes$inner_fold_metrics

# Convertir las columnas a numéricas si no lo están
inner_fold_metrics <- inner_fold_metrics %>% 
  mutate(across(c(Fold, Inner_Fold, TP, TN, FP, FN, AUC), as.numeric))

performance_metrics <- performance_metrics %>% 
  mutate(across(c(Fold, TP, TN, FP, FN, AUC), as.numeric))

# Calcular el AUC medio de train por cada outer fold
train_auc_means <- inner_fold_metrics %>%
  group_by(Fold) %>%
  summarise(Mean_Train_AUC = mean(AUC, na.rm = TRUE))

# Calcular el AUC medio de test por cada outer fold
test_auc_means <- performance_metrics %>%
  group_by(Fold) %>%
  summarise(Mean_Test_AUC = mean(AUC, na.rm = TRUE))

# Unir los resultados en un solo dataframe
auc_means <- train_auc_means %>%
  left_join(test_auc_means, by = "Fold") %>%
  gather(key = "Type", value = "Mean_AUC", Mean_Train_AUC, Mean_Test_AUC)

# Crear el barplot
ggplot(auc_means, aes(x = factor(Fold), y = Mean_AUC, fill = Type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "AUC medio de Train vs Test por Fold",
       x = "Outer Fold",
       y = "AUC Medio",
       fill = "Tipo de AUC") +
  theme_minimal()

ggplot(inner_fold_metrics, aes(x = factor(Fold), y = AUC)) +
  geom_boxplot() +
  labs(title = "Distribución de AUC por Fold",
       x = "Fold",
       y = "AUC") +
  theme_minimal()
```

```{r, message=FALSE, warning=FALSE}
library(caret)
library(glmnet)
library(e1071)
library(pROC)

double_cross_validation_naiveBayes_lasso <- function(data, target_name, outer, inner, variables) {
  set.seed(90)
  performance_metrics <- data.frame(Fold = integer(), TP = integer(), TN = integer(),
                                    FP = integer(), FN = integer(), AUC = numeric(),
                                    Best_Variables = character(), stringsAsFactors = FALSE)
  
  inner_fold_metrics <- data.frame(Fold = integer(), Inner_Fold = integer(), 
                                   TP = integer(), TN = integer(), FP = integer(), 
                                   FN = integer(), AUC = numeric(), stringsAsFactors = FALSE)
  
  # Convertir las variables del dataset con model.matrix
  target <- as.factor(data[[target_name]])  # Aseguramos que la variable de respuesta sea factor
  features <- data[, setdiff(names(data), target_name)]
  data_matrix <- as.data.frame(model.matrix(~ ., data=features)[,-1])  # Eliminar el intercepto
  data_matrix[[target_name]] <- target
  
  outer_folds <- createFolds(data_matrix[[target_name]], k = outer)
  
  for (outer_index in seq_along(outer_folds)) {
    outer_train_data <- data_matrix[-outer_folds[[outer_index]], ]
    outer_test_data <- data_matrix[outer_folds[[outer_index]], ]
    
    cat(sprintf("Outer Fold %d\n", outer_index))
    cat("Training Data - Class 0: ", sum(outer_train_data[[target_name]] == 0), "\n")
    cat("Training Data - Class 1: ", sum(outer_train_data[[target_name]] == 1), "\n")
    cat("Testing Data - Class 0: ", sum(outer_test_data[[target_name]] == 0), "\n")
    cat("Testing Data - Class 1: ", sum(outer_test_data[[target_name]] == 1), "\n")
    
    inner_folds <- createFolds(outer_train_data[[target_name]], k = inner)
    best_model <- NULL
    best_auc <- 0
    best_vars <- colnames(outer_train_data)[!colnames(outer_train_data) %in% target_name]
    
    select_vars_lasso <- function(data, target_name) {
      target <- as.numeric(data[[target_name]]) - 1  # Convertimos a numérico para Lasso
      features <- data[, setdiff(names(data), target_name)]
      X <- model.matrix(~ ., data=features)[,-1]  # Eliminamos el intercepto
      y <- target
      
      # Ajustamos el modelo Lasso
      lasso_model <- cv.glmnet(X, y, alpha=1)
      lasso_coef <- coef(lasso_model, s = "lambda.min")
      
      # Seleccionamos las variables no nulas
      selected_vars <- rownames(lasso_coef)[lasso_coef[, 1] != 0]
      selected_vars <- setdiff(selected_vars, "(Intercept)")
      cat("Selected variables by Lasso: ", paste(selected_vars, collapse=", "), "\n")
      return(selected_vars)
    }
    
    stepAUC <- function(vars, data, target_name, inner_folds) {
      best_inner_auc <- 0
      best_inner_model <- NULL
      best_inner_vars <- vars
      improved <- TRUE
      
      while (improved && length(vars) > 1) {
        improved <- FALSE
        best_auc_in_step <- best_inner_auc
        
        auc_vals <- numeric()
        
        for (inner_index in seq_along(inner_folds)) {
          inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
          inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
          
          selected_vars <- select_vars_lasso(inner_train_data, target_name)
          formula <- as.formula(paste(target_name, "~", paste(selected_vars, collapse = "+")))
          cat("Testing with formula: ", deparse(formula), "\n")
          
          result <- tryCatch({
            model <- naiveBayes(formula, data = inner_train_data)
            
            predictions <- predict(model, inner_test_data, type = "raw")[,2]
            roc_curve <- roc(inner_test_data[[target_name]], predictions)
            auc_vals[inner_index] <- roc_curve$auc
            cat("AUC for inner fold ", inner_index, ": ", roc_curve$auc, "\n")
            
          }, error = function(e) {
            cat("Error with variables: ", paste(selected_vars, collapse = ", "), "\n")
            cat("Error message: ", e$message, "\n")
            auc_vals[inner_index] <- NA  # Set to NA or some other indicator of failure
          })
        }
        
        mean_auc <- mean(auc_vals, na.rm = TRUE)
        cat("Mean AUC: ", mean_auc, "\n")
        if (!is.na(mean_auc) && mean_auc > best_auc_in_step) {
          best_auc_in_step <- mean_auc
          best_inner_model <- model
          best_inner_vars <- selected_vars
          improved <- TRUE
        }
        
        if (improved) {
          vars <- best_inner_vars
          best_inner_auc <- best_auc_in_step
          cat("Vars improved: ", paste(vars, collapse=", "), "\n")
          cat("AUC: ", best_inner_auc, "\n")
        }
      }
      
      list(model = best_inner_model, auc = best_inner_auc, vars = best_inner_vars)
    }
    
    best_result <- stepAUC(best_vars, outer_train_data, target_name, inner_folds)
    best_model <- best_result$model
    best_auc <- best_result$auc
    best_vars <- best_result$vars
    
    if (is.null(best_model)) {
      cat("Error: No valid model found for outer fold ", outer_index, "\n")
      next
    }

    # Evaluar el mejor modelo en el validation split del inner loop
    inner_auc_vals <- numeric()
    for (inner_index in seq_along(inner_folds)) {
      inner_train_data <- outer_train_data[-inner_folds[[inner_index]], ]
      inner_test_data <- outer_train_data[inner_folds[[inner_index]], ]
      
      predictions <- predict(best_model, inner_test_data, type = "raw")[,2]
      roc_curve <- roc(inner_test_data[[target_name]], predictions)
      inner_auc_vals[inner_index] <- roc_curve$auc
      cat("Inner fold ", inner_index, " AUC: ", roc_curve$auc, "\n")

      pred_class <- ifelse(predictions > 0.35, 1, 0)
      confusion <- table(Actual = inner_test_data[[target_name]], Predicted = pred_class)
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
      auc <- roc_curve$auc
      
      inner_metrics <- setNames(c(outer_index, inner_index, TP, TN, FP, FN, auc), names(inner_fold_metrics))
      inner_fold_metrics <- rbind(inner_fold_metrics, as.data.frame(t(inner_metrics)))
    }
    
    best_inner_auc <- mean(inner_auc_vals, na.rm = TRUE)
    cat("Best inner AUC: ", best_inner_auc, "\n")
    
    predictions <- predict(best_model, outer_test_data, type = "raw")[,2]
    pred_class <- ifelse(predictions > 0.35, 1, 0)  # Asignar clases basadas en umbral
    confusion <- table(Actual = outer_test_data[[target_name]], Predicted = pred_class)
    
    if (nrow(confusion) < 2 || ncol(confusion) < 2) {
      TP <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 2, confusion[2, 2], 0)
      TN <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 1, confusion[1, 1], 0)
      FP <- ifelse(nrow(confusion) >= 1 && ncol(confusion) >= 2, confusion[1, 2], 0)
      FN <- ifelse(nrow(confusion) >= 2 && ncol(confusion) >= 1, confusion[2, 1], 0)
    } else {
      TP <- confusion[2, 2]
      TN <- confusion[1, 1]
      FP <- confusion[1, 2]
      FN <- confusion[2, 1]
    }
    auc <- roc(outer_test_data[[target_name]], predictions)$auc
    
    metrics <- setNames(c(outer_index, TP, TN, FP, FN, auc, 
                          paste(best_vars, collapse = ",")), 
                        names(performance_metrics))
    performance_metrics <- rbind(performance_metrics, as.data.frame(t(metrics)))
    
    cat("Performance:\n", paste(performance_metrics), "\n")
    cat(sprintf("Confusion Matrix for Fold %d:\n", outer_index))
    print(confusion)
    cat(sprintf("Metrics for Fold %d:\n", outer_index))
    cat(sprintf("TP: %d, TN: %d, FP: %d, FN: %d, AUC: %f, Best Variables: %s\n",
                TP, TN, FP, FN, auc, paste(best_vars, collapse = ",")))
  }
  
  list(performance_metrics = performance_metrics, inner_fold_metrics = inner_fold_metrics)
}

# Uso de la función con los parámetros correspondientes
performance_naiveBayes <- double_cross_validation_naiveBayes_lasso(data = data_factor, 
                                                                   outer = 5,
                                                                   inner = 2,
                                                                   target_name = "PCR",
                                                                   variables = colnames(data_factor)[!colnames(data_factor) %in% "PCR"])

```













